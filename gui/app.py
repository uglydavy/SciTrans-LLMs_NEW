# -*- coding: utf-8 -*-
"""
SciTrans LLMs - Scientific Document Translation GUI
Version 1.0 - Clean Design
"""

import gradio as gr
import logging
import sys
import os
import json
import tempfile
import threading
import time
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime
from queue import Queue

sys.path.insert(0, str(Path(__file__).parent.parent))

# Logger for GUI module
logger = logging.getLogger(__name__)


class SciTransGUI:
    """Clean GUI with all features."""
    
    def __getattr__(self, name):
        """Defensive attribute access to prevent AttributeError for missing methods."""
        if name == '_generate_translation_preview':
            # Return a dummy function if method is missing (shouldn't happen, but defensive)
            def dummy_preview(*args, **kwargs):
                return "Preview generation method not available in this version."
            return dummy_preview
        raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")
    
    def __init__(self):
        self.config_file = Path.home() / ".scitrans" / "config.json"
        self.config_file.parent.mkdir(exist_ok=True)
        self.glossary_file = Path.home() / ".scitrans" / "glossary.json"
        self.translated_pdf_path = None
        self.source_pdf_path = None  # Store source PDF path for preview
        self.log_queue = Queue()
        self.load_config()
        self.load_glossary()  # Load persistent glossary
    
    def load_config(self):
        """Load configuration."""
        if self.config_file.exists():
            try:
                with open(self.config_file) as f:
                    self.config = json.load(f)
                # Ensure default_backend is set to deepseek if missing or invalid
                if "default_backend" not in self.config or self.config["default_backend"] not in ["cascade", "free", "ollama", "openai", "anthropic", "deepseek", "local", "libre", "argos", "huggingface"]:
                    self.config["default_backend"] = "deepseek"
            except:
                self.config = self._default_config()
        else:
            self.config = self._default_config()
        
        # Load API keys from environment variables if not in config
        env_map = {
            "openai": "OPENAI_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
            "deepseek": "DEEPSEEK_API_KEY",
            "huggingface": "HUGGINGFACE_API_KEY",
        }
        
        if "api_keys" not in self.config:
            self.config["api_keys"] = {}
        
        # Only add env vars to config if they exist and aren't already in config
        for backend, env_var in env_map.items():
            if backend not in self.config["api_keys"]:
                env_value = os.environ.get(env_var)
                if env_value and env_value.strip():
                    # Don't save env vars to config, just note they exist
                    # The _get_api_keys_table will check env vars directly
                    pass
    
    def _default_config(self):
        return {
            "dark_mode": True,
            "default_backend": "deepseek",
            "api_keys": {},
            "reranking_enabled": True,
            "masking_enabled": True,
            "cache_enabled": True,
            "max_candidates": 3,
            "context_window": 5,
            "glossary_enabled": True
        }
    
    def save_config(self):
        """Save configuration."""
        with open(self.config_file, 'w') as f:
            json.dump(self.config, f, indent=2)
    
    def _load_api_key_for_backend(self, backend: str) -> Optional[str]:
        """Load API key for a backend from GUI config, environment, or config file.
        
        Args:
            backend: Backend name
            
        Returns:
            API key if found, None otherwise
        """
        backend_lower = backend.lower()
        
        # Check GUI config first
        if hasattr(self, 'config') and self.config.get("api_keys", {}).get(backend_lower):
            return self.config["api_keys"][backend_lower]
        
        # Check environment variables
        env_mappings = {
            "openai": "OPENAI_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
            "deepseek": "DEEPSEEK_API_KEY",
            "huggingface": "HUGGINGFACE_API_KEY"
        }
        
        if backend_lower in env_mappings:
            api_key = os.getenv(env_mappings[backend_lower])
            if api_key:
                return api_key
        
        # Check config file (~/.scitrans/config.yaml)
        config_path = Path.home() / ".scitrans" / "config.yaml"
        if config_path.exists():
            try:
                from scitran.utils.config_loader import load_config
                config = load_config(str(config_path))
                api_keys = config.get("api_keys", {})
                if backend_lower in api_keys:
                    return api_keys[backend_lower]
            except Exception:
                pass
        
        return None
    
    def load_glossary(self):
        """Load persistent glossary from disk (SPRINT 3: Using GlossaryManager)."""
        from scitran.translation.glossary.manager import GlossaryManager
        
        self.glossary_manager = GlossaryManager()
        self.glossary = {}  # Keep for UI display
        
        if self.glossary_file.exists():
            try:
                count = self.glossary_manager.load_from_file(self.glossary_file)
                self.glossary = self.glossary_manager.to_dict()
                logger.info(f"Loaded {count} terms from persistent storage")
            except Exception as e:
                logger.warning(f"Could not load glossary: {e}")
                self.glossary = {}
    
    def save_glossary(self):
        """Save glossary to disk for persistence (SPRINT 3: Using GlossaryManager)."""
        if hasattr(self, 'glossary_manager') and self.glossary_manager:
            self.glossary_manager.export_to_file(Path(self.glossary_file))
        else:
            # Fallback to old method
            with open(self.glossary_file, 'w') as f:
                json.dump(self.glossary, f, indent=2)
    
    def log(self, msg: str):
        """Add log message."""
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.log_queue.put(f"[{timestamp}] {msg}")
    
    # =========================================================================
    # Translation Functions
    # =========================================================================
    
    def translate_document(
        self,
        pdf_file,
        source_lang,
        target_lang,
        backend,
        model_name,
        advanced_options,
        num_candidates,
        context_window,
        quality_threshold,
        prompt_rounds,
        batch_size,
        enable_parallel,
        max_workers,
        adaptive_concurrency,
        start_page,
        end_page,
        font_dir,
        font_files,
        font_priority,
        mask_custom_macros,
        mask_apostrophes_in_latex,
        progress=gr.Progress()
    ):
        """Translate document with proper rendering and live progress."""
        # Initialize logs and add_log BEFORE try block so they're always available in error handler
        logs = []
        def add_log(msg):
            timestamp = datetime.now().strftime("%H:%M:%S")
            logs.append(f"[{timestamp}] {msg}")
            # Also print to console for debugging
            print(f"[GUI] {timestamp} {msg}")
        
        # CRITICAL: Log function entry for debugging
        add_log("üöÄ translate_document() called")
        add_log(f"Parameters: pdf_file={pdf_file is not None}, backend={backend}, source={source_lang}, target={target_lang}")
        
        # Parse advanced options
        enable_masking = "Masking" in advanced_options
        enable_reranking = "Reranking" in advanced_options
        use_context = "Context" in advanced_options
        use_glossary = "Glossary" in advanced_options
        
        add_log(f"Advanced options: masking={enable_masking}, reranking={enable_reranking}, context={use_context}, glossary={use_glossary}")
        
        # logs and add_log are already defined at function start (lines 157-158)
        
        try:
            add_log("‚úÖ Entered try block - starting translation process")
            # Handle both file uploads and URL-downloaded PDFs
            # CRITICAL FIX: Check self.source_pdf_path if pdf_file is None (URL-loaded PDFs)
            if pdf_file is None:
                # Check if we have a URL-loaded PDF stored
                if hasattr(self, 'source_pdf_path') and self.source_pdf_path:
                    input_path = self.source_pdf_path
                    add_log(f"Using URL-loaded PDF: {Path(input_path).name}")
                else:
                    return (
                        "‚ùå Error: No PDF file provided",
                        gr.update(visible=False),
                        "Please upload a PDF file or provide a URL.",
                        None,
                        "of 0",
                        gr.update(maximum=1, value=1),
                        None,
                        "",
                        ""
                    )
            else:
                # Get PDF path - handle both file objects and string paths
                if isinstance(pdf_file, str):
                    input_path = pdf_file  # Already a path (from URL download)
                elif hasattr(pdf_file, 'name'):
                    input_path = pdf_file.name  # File upload object
                else:
                    input_path = str(pdf_file)
            
            from scitran.extraction.pdf_parser import PDFParser
            from scitran.core.pipeline import TranslationPipeline, PipelineConfig
            from scitran.rendering.pdf_renderer import PDFRenderer
            
            # Convert to Path object
            input_path = Path(input_path)
            add_log(f"Input: {input_path.name}")
            
            progress(0.05, desc="Parsing PDF...")
            add_log("Parsing PDF structure...")
            
            parser = PDFParser()
            # Page range support - handle empty strings from Gradio
            # Gradio Number components return None for empty, but we need to handle it properly
            start_page_val = 0  # Default to first page (0-based)
            if start_page is not None:
                try:
                    start_page_str = str(start_page).strip()
                    if start_page_str and start_page_str.lower() not in ['none', 'null', '']:
                        start_page_val = int(float(start_page))  # Handle both int and float inputs
                        start_page_val = max(0, start_page_val)  # Ensure non-negative
                except (ValueError, TypeError):
                    start_page_val = 0
            
            end_page_val = None  # None means process all pages
            if end_page is not None:
                try:
                    end_page_str = str(end_page).strip()
                    if end_page_str and end_page_str.lower() not in ['none', 'null', '']:
                        end_page_val = int(float(end_page))  # Handle both int and float inputs
                        # If end_page is 0, treat as "all pages" (None) - 0 means "no limit" in UI
                        if end_page_val == 0:
                            end_page_val = None
                        else:
                            end_page_val = max(start_page_val, end_page_val) if end_page_val is not None else None  # Ensure >= start_page
                except (ValueError, TypeError):
                    end_page_val = None

            document = parser.parse(
                str(input_path),
                max_pages=None,  # Process all pages (respecting start_page/end_page)
                start_page=start_page_val if start_page_val is not None else 0,
                end_page=end_page_val,  # None means process all pages from start_page
            )
            total_blocks = len(document.translatable_blocks)
            num_pages = document.stats.get("num_pages", 0)
            add_log(f"Parsed {num_pages} pages")
            add_log(f"Found {total_blocks} text blocks")
            
            # Log page-by-page block counts for debugging
            blocks_by_page = {}
            for seg in document.segments:
                for block in seg.blocks:
                    if block.bbox:
                        page = block.bbox.page
                        blocks_by_page[page] = blocks_by_page.get(page, 0) + 1
            for page, count in sorted(blocks_by_page.items()):
                add_log(f"  Page {page + 1}: {count} blocks")
            
            # Log blocks per page for debugging
            blocks_per_page = {}
            for block in document.translatable_blocks:
                if block.bbox:
                    page = block.bbox.page
                    blocks_per_page[page] = blocks_per_page.get(page, 0) + 1
            for page, count in sorted(blocks_per_page.items()):
                add_log(f"  Page {page + 1}: {count} blocks")
            
            progress(0.1, desc="Configuring pipeline...")
            add_log(f"Backend: {backend}")
            
            # Validate and normalize model name for the selected backend
            model_name = self.validate_model_for_backend(backend, model_name)
            if model_name and model_name != "default":
                add_log(f"Model: {model_name}")
            
            # Warn about free backend limitations
            if backend.lower() in ['cascade', 'free']:
                add_log(f"‚ö†Ô∏è Note: Free backends have rate limits - large PDFs will be slow")
                add_log(f"üí° Tip: For large documents, use paid backends (OpenAI/Anthropic)")
            
            add_log(f"Masking: {'ON' if enable_masking else 'OFF'}")
            add_log(f"Reranking: {'ON' if enable_reranking else 'OFF'}")
            add_log(f"Caching: ON (persistent)")
            
            # For speed, use batch mode when reranking is off
            # When reranking is on, use user-selected candidate count (turns)
            # Safely convert numeric inputs (handle empty strings from Gradio)
            def safe_int(value, default=None):
                if value is None or (isinstance(value, str) and not value.strip()):
                    return default
                try:
                    return int(value)
                except (ValueError, TypeError):
                    return default
            
            def safe_float(value, default=0.0):
                if value is None or (isinstance(value, str) and not value.strip()):
                    return default
                try:
                    return float(value)
                except (ValueError, TypeError):
                    return default
            
            # Load API key for the selected backend
            api_key = self._load_api_key_for_backend(backend)
            
            # Check if API key is required but missing
            requires_api_key = backend.lower() in ["openai", "anthropic", "deepseek", "huggingface"]
            if requires_api_key and not api_key:
                error_msg = (
                    f"‚ùå Backend '{backend}' requires an API key.\n\n"
                    f"Please set your API key:\n"
                    f"1. Go to Settings tab\n"
                    f"2. Enter your {backend.upper()}_API_KEY\n"
                    f"3. Or set environment variable: {backend.upper()}_API_KEY\n\n"
                    f"üí° Tip: Use 'free' or 'cascade' backend for testing without API keys."
                )
                return (
                    error_msg,
                    gr.update(value=None, visible=False),
                    error_msg,
                    None,
                    "of 0",
                    gr.update(maximum=1, value=1),
                    self.source_pdf_path if hasattr(self, 'source_pdf_path') and self.source_pdf_path else None,
                    "",
                    f"Error: {backend} backend requires API key. See error message above."
                )
            
            # PHASE 1.3: Check if fast mode should be enabled (no reranking + single candidate)
            fast_mode_enabled = not enable_reranking and safe_int(num_candidates, 1) == 1
            
            config = PipelineConfig(
                source_lang=source_lang,
                target_lang=target_lang,
                backend=backend,
                model_name=model_name if model_name and model_name != "default" else None,
                api_key=api_key,  # Load API key from config/env
                enable_masking=enable_masking,
                enable_reranking=enable_reranking,
                num_candidates=safe_int(num_candidates, 1) if enable_reranking else 1,
                cache_translations=True,
                enable_glossary=use_glossary,
                context_window_size=safe_int(context_window, 5),
                quality_threshold=safe_float(quality_threshold, 0.7),
                batch_size=safe_int(batch_size, 20),  # Increased default for speed
                prompt_optimization_rounds=safe_int(prompt_rounds, 0),
                optimize_prompts=safe_int(prompt_rounds, 0) > 0,
                debug_mode=True,
                debug_log_path=Path(".cache/scitrans/gui_debug.jsonl"),  # PHASE 4.1
                mask_custom_macros=bool(mask_custom_macros),
                mask_apostrophes_in_latex=bool(mask_apostrophes_in_latex),
                enable_parallel_processing=bool(enable_parallel),
                max_workers=safe_int(max_workers, None) if max_workers and str(max_workers).strip() else None,
                adaptive_concurrency=bool(adaptive_concurrency),
                fast_mode=fast_mode_enabled,  # PHASE 1.3: Auto-enable when appropriate
            )
            
            pipeline = TranslationPipeline(config)
            
            # Create progress callback for live updates with performance tracking
            perf_metrics = {
                "start_time": time.time(),
                "blocks_processed": 0,
                "cache_hits": 0,
                "blocks_per_sec": 0.0
            }
            
            def pipeline_progress(prog: float, msg: str):
                # Map pipeline progress (0-1) to our progress (0.1-0.85)
                mapped_progress = 0.1 + (prog * 0.75)
                progress(mapped_progress, desc=msg)
                add_log(msg)
                
                # Update performance metrics
                elapsed = time.time() - perf_metrics["start_time"]
                if elapsed > 0 and "blocks" in msg.lower():
                    # Try to extract block count from message
                    import re
                    match = re.search(r'(\d+)\s*/\s*(\d+)\s*blocks?', msg)
                    if match:
                        processed = int(match.group(1))
                        total = int(match.group(2))
                        perf_metrics["blocks_processed"] = processed
                        perf_metrics["blocks_per_sec"] = processed / elapsed if elapsed > 0 else 0
            
            progress(0.15, desc="Starting translation...")
            add_log("Starting translation with caching enabled...")
            add_log(f"Calling pipeline.translate_document() with {total_blocks} blocks...")
            
            # Translate with progress updates
            try:
                result = pipeline.translate_document(document, progress_callback=pipeline_progress)
                add_log(f"‚úÖ Translation completed: {result.blocks_translated} blocks translated")
            except Exception as trans_error:
                add_log(f"‚ùå Translation failed: {str(trans_error)}")
                import traceback
                add_log(f"Traceback: {traceback.format_exc()}")
                raise  # Re-raise to be caught by outer exception handler

            # Update preview with loading state during translation
            # (The actual preview will be updated after rendering)
            
            # Log cache stats if available
            stats = pipeline.get_statistics()
            if 'batch_cache_hits' in stats:
                add_log(f"Cache hits: {stats.get('batch_cache_hits', 0)}")
                add_log(f"New translations: {stats.get('batch_translated', 0)}")
            if 'cache_hits' in stats:
                add_log(f"Sequential cache hits: {stats.get('cache_hits', 0)}")
            
            add_log(f"Translated {result.blocks_translated}/{total_blocks} blocks")
            
            # Generate translation preview (text preview before rendering) - COMPLETELY INLINE, NO METHOD DEPENDENCY

            try:
                preview_lines = []
                preview_lines.append("=" * 60)
                preview_lines.append("TRANSLATION PREVIEW (Before PDF Rendering)")
                preview_lines.append("=" * 60)
                preview_lines.append("")
                preview_lines.append(f"Translation completed: {result.blocks_translated}/{total_blocks} blocks translated.\n")
                
                block_count = 0
                max_blocks = 50
                for segment in document.segments:
                    for block in segment.blocks:
                        if not hasattr(block, 'is_translatable') or (hasattr(block, 'is_translatable') and not block.is_translatable):
                            continue
                        if block_count >= max_blocks:
                            preview_lines.append(f"\n... (showing first {max_blocks} blocks)")
                            break
                        
                        block_count += 1
                        preview_lines.append(f"[Block {getattr(block, 'block_id', 'unknown')}]")
                        
                        if hasattr(block, 'source_text') and block.source_text:
                            src = str(block.source_text)[:200]
                            preview_lines.append(f"Source: {src}{'...' if len(str(block.source_text)) > 200 else ''}")
                        
                        if hasattr(block, 'translated_text') and block.translated_text:
                            trans = str(block.translated_text)[:200]
                            preview_lines.append(f"Translation: {trans}{'...' if len(str(block.translated_text)) > 200 else ''}")
                        else:
                            preview_lines.append("Translation: [NOT TRANSLATED]")
                        
                        preview_lines.append("")
                    
                    if block_count >= max_blocks:
                        break
                
                if block_count == 0:
                    translation_preview_text = f"Translation completed: {result.blocks_translated}/{total_blocks} blocks translated.\n\nNo translatable blocks found in document."
                else:
                    translation_preview_text = "\n".join(preview_lines)
                
            except Exception as preview_error:
                translation_preview_text = f"Translation completed: {result.blocks_translated}/{total_blocks} blocks translated.\n\nPreview generation error: {str(preview_error)}"
                add_log(f"‚ö†Ô∏è Preview generation failed: {preview_error}")
            
            # Debug: Check how many blocks have translations per page
            translated_by_page = {}
            translated_with_bbox = 0
            for seg in document.segments:
                for b in seg.blocks:
                    if b.translated_text and b.bbox:
                        translated_with_bbox += 1
                        page = b.bbox.page
                        translated_by_page[page] = translated_by_page.get(page, 0) + 1
            missing_blocks = [b.block_id for seg in document.segments for b in seg.blocks if not b.translated_text]
            
            add_log(f"Blocks with translation + bbox: {translated_with_bbox}")
            for page, count in sorted(translated_by_page.items()):
                add_log(f"  Page {page + 1}: {count} translated blocks")
            
            progress(0.9, desc="Rendering PDF...")
            add_log("Rendering translated PDF (clearing source text, preserving layout)...")
            
            # Save PDF to system temp directory (Gradio requirement)
            # Use tempfile to get proper temp directory that Gradio allows
            import tempfile
            temp_output_dir = Path(tempfile.gettempdir()) / "scitrans"
            temp_output_dir.mkdir(exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            temp_output_path = temp_output_dir / f"{input_path.stem}_{target_lang}_{timestamp}.pdf"

            renderer = PDFRenderer(
                font_dir=font_dir if font_dir else None,
                font_files=[f.strip() for f in font_files.split(",") if f.strip()] if font_files else None,
                font_priority=[p.strip().lower() for p in font_priority.split(",") if p.strip()] if font_priority else None,
                overflow_strategy="smart",  # Smart overflow handling: expand boxes, split text, append pages (never shrink)
                min_font_size=8.0,  # Minimum readable font size (never shrink below this)
                target_lang=target_lang,  # STEP 7: Enable font resolution for non-Latin scripts
                download_fonts=True  # STEP 7: Download fonts if missing
            )

            renderer.render_with_layout(str(input_path), result.document, str(temp_output_path))
            
            
            # Count translated pages for pagination
            try:
                import fitz
                with fitz.open(str(temp_output_path)) as out_pdf:
                    translated_pages = len(out_pdf)
            except Exception as e:
                translated_pages = document.stats.get("num_pages", 1)
            
            # Also copy to persistent location for user access
            # Keep final output in temp (Gradio requirement) and optionally mirror to home cache
            persistent_output_dir = Path.home() / ".scitrans" / "output"
            persistent_output_dir.mkdir(parents=True, exist_ok=True)
            persistent_output_path = persistent_output_dir / f"{input_path.stem}_{target_lang}_{timestamp}.pdf"
            
            import shutil
            if temp_output_path.exists():
                shutil.copy2(temp_output_path, persistent_output_path)
                file_size = temp_output_path.stat().st_size / 1024  # KB
                add_log(f"‚úì PDF created: {temp_output_path.name} ({file_size:.1f} KB)")
                add_log(f"üìÅ Also saved to: {persistent_output_path}")
            else:
                add_log(f"‚ö†Ô∏è Warning: PDF file not found at {temp_output_path}")
                # Return all 9 values for error case
                return (
                    "Error: PDF not created",
                    gr.update(value=None, visible=False, interactive=False),
                    "\n".join(logs),
                    None,
                    "of 0",
                    gr.update(maximum=1, value=1),
                    None,
                    "Error: PDF file was not created. Check logs for details.",
                    "No preview available - PDF creation failed."
                )
            
            # Store paths as absolute paths for Gradio File components
            self.translated_pdf_path = str(Path(temp_output_path).resolve())
            # Keep source PDF path stored for preview (ensure absolute)
            if not self.source_pdf_path:
                self.source_pdf_path = str(Path(input_path).resolve())
            else:
                self.source_pdf_path = str(Path(self.source_pdf_path).resolve())
            
            progress(1.0, desc="Complete!")
            add_log("Translation complete!")
            
            # Calculate performance metrics
            elapsed = time.time() - perf_metrics["start_time"]
            blocks_per_sec = total_blocks / elapsed if elapsed > 0 else 0
            cache_hits = stats.get('batch_cache_hits', 0) + stats.get('cache_hits', 0)
            cache_hit_rate = (cache_hits / total_blocks * 100) if total_blocks > 0 else 0
            
            # Calculate visual metrics (if both PDFs exist)
            visual_metrics_text = ""
            try:
                from scitran.evaluation.visual_metrics import compute_visual_similarity, render_pdf_page_to_image
                source_img = render_pdf_page_to_image(str(input_path), 0)
                translated_img = render_pdf_page_to_image(str(temp_output_path), 0)
                if source_img is not None and translated_img is not None:
                    visual_sim = compute_visual_similarity(source_img, translated_img)
                    if visual_sim.get("structural_similarity") is not None:
                        visual_metrics_text = f"\nVisual Similarity: {visual_sim['structural_similarity']:.2%}"
                        add_log(f"Visual SSIM: {visual_sim['structural_similarity']:.2%}")
            except Exception as e:
                logger.debug(f"Visual metrics calculation failed: {e}")
            
            # Calculate coverage and quality metrics
            coverage = result.coverage if hasattr(result, 'coverage') else (result.blocks_translated / total_blocks if total_blocks > 0 else 0.0)
            overflow_count = len(renderer.overflow_report) if hasattr(renderer, 'overflow_report') else 0
            
            status = f"‚úì Translation Complete\n"
            status += f"Blocks: {result.blocks_translated}/{total_blocks} ({coverage:.1%} coverage)\n"
            status += f"Time: {result.duration:.1f}s ({elapsed:.1f}s total)\n"
            status += f"Speed: {blocks_per_sec:.1f} blocks/sec\n"
            status += f"Backend: {backend}"
            if overflow_count > 0:
                status += f"\nOverflow events: {overflow_count}"
            if visual_metrics_text:
                status += visual_metrics_text
            if cache_hits > 0:
                status += f"\nCache: {cache_hits} hits ({cache_hit_rate:.1f}% hit rate)"
            
            # Add scoring information if available
            score_text = ""
            if hasattr(result, 'score_report') and result.score_report:
                score_report = result.score_report
                score_text = "\n\nüìä Translation Quality Scores:\n"
                score_text += f"‚Ä¢ Overall: {score_report.avg_overall:.1%}\n"
                score_text += f"‚Ä¢ Fluency: {score_report.avg_fluency:.1%}\n"
                score_text += f"‚Ä¢ Adequacy: {score_report.avg_adequacy:.1%}\n"
                score_text += f"‚Ä¢ Glossary: {score_report.avg_glossary:.1%}\n"
                score_text += f"‚Ä¢ Format: {score_report.avg_format:.1%}\n"
                score_text += f"‚Ä¢ Numeric: {score_report.avg_numeric:.1%}\n"
                score_text += f"\nQuality Distribution:\n"
                score_text += f"‚Ä¢ High (‚â•0.8): {score_report.high_quality_blocks} blocks\n"
                score_text += f"‚Ä¢ Medium (0.5-0.8): {score_report.medium_quality_blocks} blocks\n"
                score_text += f"‚Ä¢ Low (<0.5): {score_report.low_quality_blocks} blocks\n"
                if score_report.issues:
                    high_severity_issues = [i for i in score_report.issues if i.get('severity') == 'high']
                    if high_severity_issues:
                        score_text += f"\n‚ö†Ô∏è {len(high_severity_issues)} high-severity issues detected"
            status += score_text
            
            # Performance info
            perf_text = f"Performance Metrics:\n"
            perf_text += f"‚Ä¢ Throughput: {blocks_per_sec:.2f} blocks/second\n"
            perf_text += f"‚Ä¢ Cache hit rate: {cache_hit_rate:.1f}%\n"
            perf_text += f"‚Ä¢ Total time: {elapsed:.2f}s\n"
            if 'batch_translated' in stats:
                perf_text += f"‚Ä¢ Batch translated: {stats.get('batch_translated', 0)}\n"
                perf_text += f"‚Ä¢ Batch cached: {stats.get('batch_cache_hits', 0)}"
            
            
            # Return PDF file path for download button, images for preview
            page_update = gr.update(maximum=max(1, translated_pages), value=1)
            page_total_text = f"of {max(1, translated_pages)}"
            
            # Ensure paths are absolute
            translated_path_str = str(Path(temp_output_path).resolve())
            source_path_str = str(Path(self.source_pdf_path).resolve()) if self.source_pdf_path else (str(Path(input_path).resolve()) if input_path else None)
            
            # Render first pages as images for preview
            source_img = self.render_pdf_page(source_path_str, 0) if source_path_str else None
            trans_img = self.render_pdf_page(translated_path_str, 0) if translated_path_str else None
            
            # Generate score display text
            score_display_text = ""
            if hasattr(result, 'score_report') and result.score_report:
                score_report = result.score_report
                score_display_text = "üìä Translation Quality Scores\n"
                score_display_text += "=" * 50 + "\n\n"
                score_display_text += f"Overall Score: {score_report.avg_overall:.1%}\n\n"
                score_display_text += "Dimension Scores:\n"
                score_display_text += f"  ‚Ä¢ Fluency:     {score_report.avg_fluency:.1%}\n"
                score_display_text += f"  ‚Ä¢ Adequacy:    {score_report.avg_adequacy:.1%}\n"
                score_display_text += f"  ‚Ä¢ Glossary:    {score_report.avg_glossary:.1%}\n"
                score_display_text += f"  ‚Ä¢ Format:      {score_report.avg_format:.1%}\n"
                score_display_text += f"  ‚Ä¢ Numeric:     {score_report.avg_numeric:.1%}\n\n"
                score_display_text += "Quality Distribution:\n"
                total_translated = score_report.translated_blocks or 1
                score_display_text += f"  ‚Ä¢ High (‚â•0.8):  {score_report.high_quality_blocks} blocks ({score_report.high_quality_blocks / total_translated:.1%})\n"
                score_display_text += f"  ‚Ä¢ Medium (0.5-0.8): {score_report.medium_quality_blocks} blocks ({score_report.medium_quality_blocks / total_translated:.1%})\n"
                score_display_text += f"  ‚Ä¢ Low (<0.5):   {score_report.low_quality_blocks} blocks ({score_report.low_quality_blocks / total_translated:.1%})\n\n"
                if score_report.issues:
                    high_severity = [i for i in score_report.issues if i.get('severity') == 'high']
                    medium_severity = [i for i in score_report.issues if i.get('severity') == 'medium']
                    score_display_text += f"Issues Detected:\n"
                    score_display_text += f"  ‚Ä¢ High severity: {len(high_severity)}\n"
                    score_display_text += f"  ‚Ä¢ Medium severity: {len(medium_severity)}\n"
                    score_display_text += f"  ‚Ä¢ Total: {len(score_report.issues)}\n\n"
                    if high_severity:
                        score_display_text += "High Severity Issues:\n"
                        for issue in high_severity[:5]:
                            score_display_text += f"  ‚ö†Ô∏è {issue.get('message', '')[:60]}\n"
            else:
                score_display_text = "No scoring data available. Scores are computed automatically after translation."
            
            return (
                status,
                gr.update(value=translated_path_str, visible=True, interactive=True),  # Download button gets file path
                "\n".join(logs),
                trans_img,  # Return rendered image for translated preview
                page_total_text,
                page_update,
                source_img,  # Return rendered image for source preview
                perf_text,
                translation_preview_text,
                score_display_text,  # Score information
            )
            
        except Exception as e:
            import traceback
            from scitran.core.exceptions import SciTransError
            
            # CRITICAL: Ensure logs and add_log are available (they should be, but defensive check)
            if 'logs' not in locals():
                logs = []
            if 'add_log' not in locals():
                def add_log(msg):
                    timestamp = datetime.now().strftime("%H:%M:%S")
                    logs.append(f"[{timestamp}] {msg}")
            
            error_msg = str(e)
            error_details = ""
            
            # Enhanced error display with suggestions
            try:
                if isinstance(e, SciTransError):
                    error_msg = e.message
                    if e.suggestion:
                        error_details = f"\n\nüí° Suggestion: {e.suggestion}"
                    if e.recoverable:
                        error_details += "\n\nThis error is recoverable. You can try again with different settings."
                
                add_log(f"‚ùå Error: {error_msg}")
                if error_details:
                    add_log(error_details)
            except:
                # If add_log fails, at least set error_msg
                pass
            
            # Enhanced error logging with full traceback
            try:
                import traceback as tb
                full_traceback = tb.format_exc()
            except:
                pass
            
            try:
                full_error = f"‚ùå Error: {error_msg}{error_details}"
                if hasattr(self, 'config') and self.config.get("debug_mode", False):
                    full_error += f"\n\nDebug traceback:\n{full_traceback if 'full_traceback' in locals() else 'N/A'}"
            except:
                full_error = f"‚ùå Error: {error_msg}"
            
            # Generate error preview - always succeed
            try:
                translation_preview_text = f"Error: {error_msg}\n\nNo translation preview available due to error."
            except:
                translation_preview_text = "Error generating preview"
            
            # ALWAYS return exactly 9 values, even if something fails
            try:
                logs_str = "\n".join(logs) if logs else "No logs available"
            except:
                logs_str = "Error generating logs"
            
            try:
                perf_text = f"Error occurred. Check logs for details.\n\nError: {error_msg}"
            except:
                perf_text = "Error occurred. Check logs for details."
            
            # Ensure source PDF path is available for preview even on error
            source_path_str = None
            if hasattr(self, 'source_pdf_path') and self.source_pdf_path:
                try:
                    source_path_str = str(Path(self.source_pdf_path).resolve())
                except:
                    pass
            
            # If we have input_path from earlier, use it
            if not source_path_str and 'input_path' in locals():
                try:
                    source_path_str = str(Path(input_path).resolve())
                except:
                    pass
            
            # Render source PDF page as image if available
            source_img = self.render_pdf_page(source_path_str, 0) if source_path_str else None
            
            # Final return - guaranteed to return 9 values
            return (
                full_error,
                gr.update(value=None, visible=False, interactive=False),
                logs_str,
                None,  # Translated PDF image (None on error)
                "of 0",
                gr.update(maximum=1, value=1),
                source_img,  # Source PDF image (show even on error)
                perf_text,
                translation_preview_text
            )
    
    def _generate_translation_preview(self, document, max_blocks=50):
        """Generate text preview of translated document.
        
        NOTE: This method exists for backward compatibility.
        The new inline preview generation (lines 353-400) doesn't use this method.
        """
        try:
            from scitran.core.models import Document
            
            if not isinstance(document, Document):
                return "No document available for preview."
            
            preview_lines = []
            preview_lines.append("=" * 60)
            preview_lines.append("TRANSLATION PREVIEW (Before PDF Rendering)")
            preview_lines.append("=" * 60)
            preview_lines.append("")
            
            block_count = 0
            for segment in document.segments:
                for block in segment.blocks:
                    if not block.is_translatable:
                        continue
                    
                    if block_count >= max_blocks:
                        preview_lines.append(f"\n... (showing first {max_blocks} blocks)")
                        break
                    
                    block_count += 1
                    preview_lines.append(f"[Block {block.block_id}]")
                    
                    if block.source_text:
                        preview_lines.append(f"Source: {block.source_text[:200]}{'...' if len(block.source_text) > 200 else ''}")
                    
                    if block.translated_text:
                        preview_lines.append(f"Translation: {block.translated_text[:200]}{'...' if len(block.translated_text) > 200 else ''}")
                    else:
                        preview_lines.append("Translation: [NOT TRANSLATED]")
                    
                    preview_lines.append("")
                
                if block_count >= max_blocks:
                    break
            
            if block_count == 0:
                return "No translatable blocks found in document."
            
            return "\n".join(preview_lines)
        except Exception as e:
            return f"Error generating preview: {str(e)}"
    
    def preview_pdf(self, pdf_file, page_num=1):
        """Preview PDF page - returns PDF file path for File component."""
        if pdf_file is None:
            return None
        try:
            # Return the PDF file path directly - Gradio File component will render PDFs natively
            pdf_path = pdf_file.name if hasattr(pdf_file, 'name') else str(pdf_file)
            if isinstance(pdf_path, str) and pdf_path.endswith('.pdf'):
                return pdf_path
            return None
        except Exception as e:
            print(f"Preview error: {e}")
            return None
    
    def create_loading_image(self, message="Translating..."):
        """Create a loading overlay image."""
        from PIL import Image, ImageDraw, ImageFont
        
        # Create a semi-transparent overlay
        img = Image.new('RGBA', (800, 500), (0, 0, 0, 128))  # Semi-transparent black
        draw = ImageDraw.Draw(img)
        
        # Try to use a nice font
        try:
            font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 24)
        except:
            font = ImageFont.load_default()
        
        # Get text size and center it
        bbox = draw.textbbox((0, 0), message, font=font)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]
        position = ((800 - text_width) // 2, (500 - text_height) // 2)
        
        # Draw white text
        draw.text(position, message, fill=(255, 255, 255, 255), font=font)
        
        return img
    
    def get_page_count(self, pdf_file):
        """Get PDF page count."""
        if pdf_file is None:
            return 1
        try:
            import fitz
            pdf_path = pdf_file.name if hasattr(pdf_file, 'name') else str(pdf_file)
            doc = fitz.open(pdf_path)
            count = len(doc)
            doc.close()
            return count
        except:
            return 1
    
    def render_pdf_page(self, pdf_path: Optional[str], page_num: int = 0):
        """
        Render a PDF page to numpy array for Gradio Image preview.
        
        Args:
            pdf_path: Path to PDF file
            page_num: Page number (0-indexed)
            
        Returns:
            numpy array (H, W, 3) or None if error
        """
        if not pdf_path or not Path(pdf_path).exists():
            return None
        
        try:
            import fitz
            import numpy as np
            
            with fitz.open(pdf_path) as doc:
                if page_num < 0 or page_num >= len(doc):
                    page_num = 0
                
                page = doc[page_num]
                # Render at 150 DPI for good quality
                pix = page.get_pixmap(matrix=fitz.Matrix(150/72, 150/72))
                
                # Convert to numpy array (Gradio Image accepts numpy arrays)
                # pix.samples is a bytes object, shape is (height, width, n)
                img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(
                    pix.height, pix.width, pix.n
                )
                
                # If RGBA, convert to RGB (Gradio prefers RGB)
                if pix.n == 4:
                    # Remove alpha channel
                    img_array = img_array[:, :, :3]
                elif pix.n == 1:
                    # Grayscale to RGB
                    img_array = np.stack([img_array[:, :, 0]] * 3, axis=2)
                
                return img_array
        except Exception as e:
            logger.error(f"Error rendering PDF page {page_num} from {pdf_path}: {e}")
            return None
    
    def download_pdf_from_url(self, url):
        """Download PDF from URL and return file path."""
        if not url or not url.strip():
            return None, "Please enter a valid URL."
        
        try:
            import requests
            import tempfile
            from pathlib import Path
            
            # Validate URL
            if not url.startswith(('http://', 'https://')):
                return None, "‚ùå Invalid URL. Must start with http:// or https://"
            
            # Download PDF
            response = requests.get(url, timeout=30, stream=True)
            response.raise_for_status()
            
            # Check content type
            content_type = response.headers.get('content-type', '').lower()
            if 'pdf' not in content_type and not url.lower().endswith('.pdf'):
                # Still try if URL ends with .pdf
                if not url.lower().endswith('.pdf'):
                    return None, f"‚ùå URL does not appear to be a PDF (content-type: {content_type})"
            
            # Save to temporary file
            temp_dir = Path.home() / ".scitrans" / "temp"
            temp_dir.mkdir(parents=True, exist_ok=True)
            
            # Generate filename from URL
            filename = url.split('/')[-1] or "downloaded.pdf"
            if not filename.endswith('.pdf'):
                filename += '.pdf'
            
            temp_path = temp_dir / filename
            
            # Download file
            with open(temp_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # Verify it's a valid PDF
            try:
                import fitz
                doc = fitz.open(str(temp_path))
                page_count = len(doc)
                doc.close()
                
                return str(temp_path), f"‚úÖ Downloaded PDF successfully ({page_count} pages)"
            except Exception as e:
                temp_path.unlink(missing_ok=True)
                return None, f"‚ùå Downloaded file is not a valid PDF: {str(e)}"
                
        except requests.exceptions.RequestException as e:
            return None, f"‚ùå Failed to download PDF: {str(e)}"
        except Exception as e:
            return None, f"‚ùå Error: {str(e)}"
    
    def _get_model_options_for_backend(self, backend: str):
        """Return model choices, default value, and visibility for a backend."""
        model_options = {
            "ollama": ["llama3.1", "llama3.2", "qwen2.5", "mistral", "gemma2", "llama3.3"],
            "huggingface": ["facebook/mbart-large-50-many-to-many-mmt", "Helsinki-NLP/opus-mt-en-fr"],
            "openai": ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo"],
            "anthropic": ["claude-3-5-sonnet-20241022", "claude-3-opus-20240229", "claude-3-sonnet-20240229"],
            "deepseek": ["deepseek-chat", "deepseek-coder"],
            "cascade": ["default"],
            "free": ["default"],
            "libre": ["default"],
            "argos": ["default"],
            "local": ["default"]
        }
        
        options = model_options.get(backend, ["default"])
        default_value = options[0]
        # Show model selector for backends that support model selection
        visible = backend in ["ollama", "huggingface", "openai", "anthropic", "deepseek"]
        return {"choices": options, "value": default_value, "visible": visible}
    
    def update_model_options(self, backend):
        """Update model dropdown options based on selected backend."""
        opts = self._get_model_options_for_backend(backend)
        # Always set value to first valid option to prevent "default" from persisting
        # when switching to backends that don't support "default"
        return gr.update(choices=opts["choices"], value=opts["value"], visible=opts["visible"])
    
    def validate_model_for_backend(self, backend, model_name):
        """Validate and normalize model name for the selected backend."""
        opts = self._get_model_options_for_backend(backend)
        valid_choices = opts["choices"]
        
        # If model is "default" or not in valid choices, use the first valid option
        if model_name == "default" or model_name not in valid_choices:
            return opts["value"]
        return model_name
    
    # =========================================================================
    # Testing Functions
    # =========================================================================
    
    def test_backend(self, backend, sample_text):
        """Test translation backend."""
        if not sample_text.strip():
            sample_text = "Machine learning enables computers to learn from data without explicit programming."
        
        try:
            from scitran.translation.base import TranslationRequest
            
            if backend == "cascade":
                from scitran.translation.backends.cascade_backend import CascadeBackend
                translator = CascadeBackend()
            elif backend == "free":
                from scitran.translation.backends.free_backend import FreeBackend
                translator = FreeBackend()
            elif backend == "ollama":
                from scitran.translation.backends.ollama_backend import OllamaBackend
                translator = OllamaBackend()
            elif backend == "local":
                from scitran.translation.backends.local_backend import LocalBackend
                translator = LocalBackend()
            elif backend == "libre":
                from scitran.translation.backends.libre_backend import LibreBackend
                translator = LibreBackend()
            elif backend == "argos":
                from scitran.translation.backends.argos_backend import ArgosBackend
                translator = ArgosBackend()
            elif backend == "huggingface":
                from scitran.translation.backends.huggingface_backend import HuggingFaceBackend
                translator = HuggingFaceBackend()
            else:
                api_key = self.config.get("api_keys", {}).get(backend)
                if not api_key:
                    return f"Backend '{backend}' requires API key. Set it in Settings."
                
                if backend == "openai":
                    from scitran.translation.backends.openai_backend import OpenAIBackend
                    translator = OpenAIBackend(api_key=api_key)
                elif backend == "anthropic":
                    from scitran.translation.backends.anthropic_backend import AnthropicBackend
                    translator = AnthropicBackend(api_key=api_key)
                elif backend == "deepseek":
                    from scitran.translation.backends.deepseek_backend import DeepSeekBackend
                    translator = DeepSeekBackend(api_key=api_key)
                else:
                    return f"Unknown backend: {backend}"
            
            if not translator.is_available():
                return f"Backend '{backend}' is not available."
            
            request = TranslationRequest(text=sample_text, source_lang="en", target_lang="fr")
            
            start = time.time()
            response = translator.translate_sync(request)
            elapsed = time.time() - start
            
            if response.translations:
                result = f"Backend '{backend}' OK ({elapsed:.2f}s)\n\n"
                result += f"EN: {sample_text}\n\n"
                result += f"FR: {response.translations[0]}"
                return result
            else:
                return f"Backend returned no translation."
                
        except Exception as e:
            return f"Error: {str(e)}"
    
    def test_masking(self, test_input):
        """Test masking with custom or default input."""
        if not test_input.strip():
            test_input = "The equation $E=mc^2$ is famous. See https://arxiv.org for more."
        
        try:
            from scitran.masking.engine import MaskingEngine
            from scitran.core.models import Block
            
            engine = MaskingEngine()
            block = Block(block_id="test", source_text=test_input)
            masked = engine.mask_block(block)
            
            result = f"Original: {test_input}\n\n"
            result += f"Masked: {masked.masked_text}\n\n"
            result += f"Masks found: {len(masked.masks)}\n"
            for m in masked.masks:
                result += f"  ‚Ä¢ {m.mask_type}: '{m.original}' ‚Üí '{m.placeholder}'\n"
            
            # Show what would be sent to translator
            result += f"\n(The masked text above is what gets translated, preserving formulas/URLs)"
            
            return result
        except Exception as e:
            return f"Error: {str(e)}"
    
    def test_layout(self, pdf_file):
        """Test layout extraction with detailed analysis."""
        if pdf_file is None:
            return "Default test: Layout extraction module OK.\n\nUpload a PDF to test with your document."
        
        try:
            import fitz
            from scitran.core.models import BlockType
            from scitran.extraction.pdf_parser import PDFParser
            
            pdf_path = pdf_file.name if hasattr(pdf_file, 'name') else str(pdf_file)
            doc = fitz.open(pdf_path)
            parser = PDFParser()
            
            result = f"PDF: {Path(pdf_path).name}\n"
            result += f"Total Pages: {len(doc)}\n"
            result += "=" * 60 + "\n\n"
            
            # Analyze first 3 pages in detail
            for i, page in enumerate(doc[:3]):
                result += f"Page {i+1}:\n"
                result += f"  Size: {page.rect.width:.0f} x {page.rect.height:.0f}\n"
                
                # Images
                images = page.get_images()
                result += f"  Images: {len(images)}\n"
                
                # Extract text with detailed info
                text_dict = page.get_text("dict", flags=fitz.TEXT_PRESERVE_WHITESPACE | fitz.TEXT_PRESERVE_LIGATURES)
                
                # Analyze fonts
                fonts_used = {}
                block_types = {}
                tables_count = 0
                equations_count = 0
                figures_count = 0
                
                for block_data in text_dict.get("blocks", []):
                    if "lines" not in block_data:
                        figures_count += 1
                        continue
                    
                    # Analyze fonts in this block
                    for line in block_data.get("lines", []):
                        for span in line.get("spans", []):
                            font_name = span.get("font", "unknown")
                            font_size = span.get("size", 11)
                            flags = span.get("flags", 0)
                            
                            # Parse font style
                            is_bold = bool(flags & 16)
                            is_italic = bool(flags & 2)
                            style = ""
                            if is_bold and is_italic:
                                style = "Bold+Italic"
                            elif is_bold:
                                style = "Bold"
                            elif is_italic:
                                style = "Italic"
                            else:
                                style = "Regular"
                            
                            font_key = f"{font_name} ({style}, {font_size:.1f}pt)"
                            fonts_used[font_key] = fonts_used.get(font_key, 0) + 1
                    
                    # Classify block type
                    block_text = ""
                    for line in block_data.get("lines", []):
                        for span in line.get("spans", []):
                            block_text += span.get("text", "")
                    
                    if block_text.strip():
                        # Use parser's classification
                        classified = parser._classify_block(block_text)
                        block_types[classified] = block_types.get(classified, 0) + 1
                        
                        if classified == "table":
                            tables_count += 1
                        elif classified == "math_content":
                            equations_count += 1
                
                # Count text blocks
                text_blocks = [b for b in text_dict.get("blocks", []) if "lines" in b]
                result += f"  Text blocks: {len(text_blocks)}\n"
                
                # Font summary
                if fonts_used:
                    result += f"  Fonts used ({len(fonts_used)} unique):\n"
                    # Sort by usage count
                    sorted_fonts = sorted(fonts_used.items(), key=lambda x: x[1], reverse=True)
                    for font_name, count in sorted_fonts[:5]:  # Top 5 fonts
                        result += f"    ‚Ä¢ {font_name}: {count} spans\n"
                    if len(sorted_fonts) > 5:
                        result += f"    ... and {len(sorted_fonts) - 5} more\n"
                
                # Block types
                if block_types:
                    result += f"  Block types:\n"
                    for block_type, count in sorted(block_types.items(), key=lambda x: x[1], reverse=True):
                        result += f"    ‚Ä¢ {block_type}: {count}\n"
                
                # Special elements
                result += f"  Special elements:\n"
                result += f"    ‚Ä¢ Tables detected: {tables_count}\n"
                result += f"    ‚Ä¢ Equations/formulas: {equations_count}\n"
                result += f"    ‚Ä¢ Figures/images: {figures_count}\n"
                
                result += "\n"
            
            doc.close()
            return result
        except Exception as e:
            import traceback
            return f"Error: {str(e)}\n\n{traceback.format_exc()}"
    
    def test_cache(self):
        """Test cache functionality."""
        try:
            from scitran.utils.fast_translator import PersistentCache
            cache = PersistentCache()
            
            cache.set("test_key", "en", "fr", "test_value")
            result = cache.get("test_key", "en", "fr")
            
            if result == "test_value":
                stats = cache.stats()
                return f"Cache OK\n\nStats: {stats}"
            else:
                return "Cache read failed"
        except Exception as e:
            return f"Error: {str(e)}"
    
    # =========================================================================
    # Glossary Functions - Extensive Domain Glossaries
    # =========================================================================
    
    def _get_scientific_ml_glossary(self, direction="en-fr"):
        """Machine Learning and AI scientific terms."""
        en_fr = {
            # Core ML concepts
            # Architecture terms
            # NLP terms
            # Computer Vision
        }
        if direction == "fr-en":
            return {v: k for k, v in en_fr.items()}
        return en_fr
    
    def _get_scientific_physics_glossary(self, direction="en-fr"):
        """Physics and mathematics scientific terms."""
        en_fr = {
            # Physics
            # Mathematics
        }
        if direction == "fr-en":
            return {v: k for k, v in en_fr.items()}
        return en_fr
    
    def _get_scientific_bio_glossary(self, direction="en-fr"):
        """Biology and medical scientific terms."""
        en_fr = {
            # Biology
            # Structural Biology
            # Medical
        }
        if direction == "fr-en":
            return {v: k for k, v in en_fr.items()}
        return en_fr
    
    def _get_europarl_glossary(self, direction="en-fr"):
        """European Parliament and legal terms."""
        en_fr = {
                "European Union": "Union europ√©enne",
                "European Parliament": "Parlement europ√©en",
                "European Commission": "Commission europ√©enne",
                "Member State": "√âtat membre",
                "Council of the European Union": "Conseil de l'Union europ√©enne",
                "legislation": "l√©gislation",
                "regulation": "r√®glement",
                "directive": "directive",
                "treaty": "trait√©",
                "amendment": "amendement",
                "resolution": "r√©solution",
                "committee": "commission",
                "rapporteur": "rapporteur",
                "codecision": "cod√©cision",
                "subsidiarity": "subsidiarit√©",
            "human rights": "droits de l'homme",
            "rule of law": "√©tat de droit",
            "democracy": "d√©mocratie",
            "transparency": "transparence",
            "accountability": "responsabilit√©",
        }
        if direction == "fr-en":
            return {v: k for k, v in en_fr.items()}
        return en_fr
    
    def _get_chemistry_glossary(self, direction="en-fr"):
        """Chemistry scientific terms."""
        en_fr = {
            "molecule": "mol√©cule",
            "atom": "atome",
            "electron": "√©lectron",
            "proton": "proton",
            "neutron": "neutron",
            "chemical bond": "liaison chimique",
            "covalent bond": "liaison covalente",
            "ionic bond": "liaison ionique",
            "oxidation": "oxydation",
            "reduction": "r√©duction",
            "catalyst": "catalyseur",
            "reagent": "r√©actif",
            "solvent": "solvant",
            "solution": "solution",
            "concentration": "concentration",
            "molar mass": "masse molaire",
            "equilibrium": "√©quilibre",
            "reaction rate": "vitesse de r√©action",
            "organic chemistry": "chimie organique",
            "inorganic chemistry": "chimie inorganique",
            "polymer": "polym√®re",
            "compound": "compos√©",
            "element": "√©l√©ment",
            "isotope": "isotope",
            "valence": "valence",
            "electronegativity": "√©lectron√©gativit√©",
            "spectroscopy": "spectroscopie",
            "chromatography": "chromatographie",
            "titration": "titrage",
            "pH": "pH",
        }
        if direction == "fr-en":
            return {v: k for k, v in en_fr.items()}
        return en_fr
    
    def _get_cs_glossary(self, direction="en-fr"):
        """Computer Science terms."""
        en_fr = {
            "algorithm": "algorithme",
            "data structure": "structure de donn√©es",
            "array": "tableau",
            "linked list": "liste cha√Æn√©e",
            "hash table": "table de hachage",
            "binary tree": "arbre binaire",
            "graph": "graphe",
            "recursion": "r√©cursion",
            "iteration": "it√©ration",
            "complexity": "complexit√©",
            "time complexity": "complexit√© temporelle",
            "space complexity": "complexit√© spatiale",
            "database": "base de donn√©es",
            "query": "requ√™te",
            "index": "index",
            "cache": "cache",
            "memory": "m√©moire",
            "stack": "pile",
            "queue": "file",
            "heap": "tas",
            "sorting": "tri",
            "searching": "recherche",
            "compiler": "compilateur",
            "interpreter": "interpr√©teur",
            "operating system": "syst√®me d'exploitation",
            "network": "r√©seau",
            "protocol": "protocole",
            "encryption": "chiffrement",
            "decryption": "d√©chiffrement",
            "authentication": "authentification",
            "authorization": "autorisation",
            "API": "API",
            "framework": "cadriciel",
            "library": "biblioth√®que",
            "version control": "contr√¥le de version",
            "debugging": "d√©bogage",
            "testing": "test",
            "deployment": "d√©ploiement",
        }
        if direction == "fr-en":
            return {v: k for k, v in en_fr.items()}
        return en_fr
    
    def _get_statistics_glossary(self, direction="en-fr"):
        """Statistics terms."""
        en_fr = {
            "mean": "moyenne",
            "median": "m√©diane",
            "mode": "mode",
            "variance": "variance",
            "standard deviation": "√©cart-type",
            "distribution": "distribution",
            "normal distribution": "distribution normale",
            "probability": "probabilit√©",
            "hypothesis": "hypoth√®se",
            "null hypothesis": "hypoth√®se nulle",
            "p-value": "valeur p",
            "confidence interval": "intervalle de confiance",
            "sample": "√©chantillon",
            "population": "population",
            "correlation": "corr√©lation",
            "regression": "r√©gression",
            "linear regression": "r√©gression lin√©aire",
            "outlier": "valeur aberrante",
            "bias": "biais",
            "significance": "signification",
            "statistical significance": "significativit√© statistique",
            "Bayesian": "bay√©sien",
            "frequentist": "fr√©quentiste",
            "maximum likelihood": "maximum de vraisemblance",
        }
        if direction == "fr-en":
            return {v: k for k, v in en_fr.items()}
        return en_fr
    
    def load_glossary_domain(self, domain, direction="en-fr", progress=None):
        """Load glossary by domain (SPRINT 3: Using GlossaryManager).
        
        Now runs in a way that doesn't block the UI thread.
        """
        try:
            if progress:
                progress(0.1, f"Loading {domain} glossary...")
            
            if not hasattr(self, 'glossary_manager'):
                from scitran.translation.glossary.manager import GlossaryManager
                self.glossary_manager = GlossaryManager()
            
            if progress:
                progress(0.3, f"Parsing {domain} terms...")
            
            prev_count = len(self.glossary_manager)
            count = self.glossary_manager.load_domain(domain, direction)
            new_count = len(self.glossary_manager) - prev_count
            
            if count == 0:
                # Try alternative file naming
                alt_direction = direction.replace('-', '_')
                count = self.glossary_manager.load_domain(domain, alt_direction)
                new_count = len(self.glossary_manager) - prev_count
            
            if progress:
                progress(0.7, "Updating glossary cache...")
            
            if count == 0:
                return f"‚ö†Ô∏è Could not load {domain} glossary. File may be missing or corrupted.", self._get_glossary_preview()
            
            # Update UI glossary dict (create copy to avoid thread issues)
            self.glossary = dict(self.glossary_manager.to_dict())
            self.save_glossary()
            
            if progress:
                progress(1.0, "Done!")
            
            return f"‚úì Loaded {count} {domain.upper()} terms ({new_count} new)", self._get_glossary_preview()
        except Exception as e:
            logger.error(f"Error loading glossary domain {domain}: {e}")
            return f"‚ùå Error loading {domain}: {str(e)}", self._get_glossary_preview()
    
    def load_all_scientific_glossaries(self, direction="en-fr", progress=None):
        """Load all scientific glossaries at once (SPRINT 3: Using GlossaryManager).
        
        Now reports progress to prevent UI appearing frozen.
        """
        try:
            if not hasattr(self, 'glossary_manager'):
                from scitran.translation.glossary.manager import GlossaryManager
                self.glossary_manager = GlossaryManager()
            
            total_before = len(self.glossary_manager)
            
            # Load all domains with progress
            all_domains = ['ml', 'physics', 'biology', 'chemistry', 'cs', 'statistics', 'europarl']
            loaded = []
            failed = []
            
            for i, domain in enumerate(all_domains):
                if progress:
                    progress((i + 1) / len(all_domains), f"Loading {domain}...")
                
                try:
                    count = self.glossary_manager.load_domain(domain, direction)
                    if count > 0:
                        loaded.append(domain)
                except Exception as e:
                    logger.warning(f"Could not load {domain}: {e}")
                    failed.append(domain)
            
            total_after = len(self.glossary_manager)
            new_terms = total_after - total_before
            
            # Update UI glossary dict (create copy to avoid thread issues)
            self.glossary = dict(self.glossary_manager.to_dict())
            self.save_glossary()
            
            if progress:
                progress(1.0, "Done!")
            
            status = f"‚úì Loaded {len(loaded)} glossaries: {total_after} total terms ({new_terms} new)"
            if failed:
                status += f"\n‚ö†Ô∏è Failed: {', '.join(failed)}"
            
            return status, self._get_glossary_preview()
        except Exception as e:
            logger.error(f"Error loading all glossaries: {e}")
            return f"‚ùå Error: {str(e)}", self._get_glossary_preview()
    
    def _get_glossary_preview(self):
        """Get glossary preview for UI (thread-safe)."""
        if self.glossary:
            return [[k, v] for k, v in list(self.glossary.items())[:50]]
        return []
    
    def load_glossary_file(self, file, progress=None):
        """Load glossary from uploaded file.
        
        Now reports progress for large files.
        """
        if file is None:
            return "No file selected", self._get_glossary_preview()
        
        try:
            if progress:
                progress(0.2, "Reading file...")
            
            file_path = file.name if hasattr(file, 'name') else str(file)
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if progress:
                progress(0.5, "Processing terms...")
            
            prev_count = len(self.glossary)
            if "terms" in data:
                self.glossary.update(data["terms"])
            else:
                self.glossary.update(data)
            
            if progress:
                progress(0.8, "Saving...")
            
            new_count = len(self.glossary) - prev_count
            self.save_glossary()  # Save persistently
            
            if progress:
                progress(1.0, "Done!")
            
            return f"‚úì Loaded from file: {new_count} new terms (total: {len(self.glossary)})", self._get_glossary_preview()
        except Exception as e:
            logger.error(f"Error loading glossary file: {e}")
            return f"‚ùå Error: {str(e)}", self._get_glossary_preview()
    
    def add_glossary_term(self, source, target):
        """Add term to glossary."""
        if not source or not target:
            return "‚ö† Enter both source and target terms", self.glossary
        
        source = source.strip()
        target = target.strip()
        
        if source in self.glossary:
            self.glossary[source] = target
            self.save_glossary()  # Save persistently
            return f"‚úì Updated: '{source}' ‚Üí '{target}'", self.glossary
        else:
            self.glossary[source] = target
            self.save_glossary()  # Save persistently
            return f"‚úì Added: '{source}' ‚Üí '{target}'", self.glossary
    
    def clear_glossary(self):
        """Clear glossary."""
        count = len(self.glossary)
        self.glossary = {}
        self.save_glossary()  # Save persistently
        return f"‚úì Cleared {count} terms", {}
    
    def load_online_glossary(self, source, direction="en-fr"):
        """Load glossary from online sources like HuggingFace, Europarl, etc."""
        import requests
        
        prev_count = len(self.glossary)
        
        try:
            if source == "europarl_full":
                # Europarl parallel corpus - fetch common terms
                # Using a subset available via GitHub
                url = "https://raw.githubusercontent.com/Wikipedia-translations/europarl-extract/main/en-fr.json"
                try:
                    response = requests.get(url, timeout=15)
                    if response.status_code == 200:
                        data = response.json()
                        if isinstance(data, dict):
                            self.glossary.update(data)
                except:
                    # Fallback: use expanded built-in Europarl
                    self.glossary.update(self._get_expanded_europarl_glossary(direction))
            
            elif source == "huggingface_opus":
                # HuggingFace OPUS-100 terms - common scientific terms
                # Using a curated subset
                url = "https://huggingface.co/datasets/Helsinki-NLP/opus-100/raw/main/README.md"
                # Since direct API access may be limited, use expanded built-in
                self.glossary.update(self._get_expanded_scientific_glossary(direction))
            
            elif source == "wiktionary":
                # Wiktionary translations (curated subset)
                self.glossary.update(self._get_wiktionary_terms(direction))
            
            elif source == "iate":
                # IATE - Inter-Active Terminology for Europe
                self.glossary.update(self._get_iate_terms(direction))
            
            else:
                return f"Unknown source: {source}", self.glossary
            
            new_count = len(self.glossary) - prev_count
            self.save_glossary()
            return f"‚úì Loaded from {source}: {new_count} new terms (total: {len(self.glossary)})", self.glossary
            
        except Exception as e:
            return f"Error loading from {source}: {str(e)}", self.glossary
    
    def _get_expanded_europarl_glossary(self, direction="en-fr"):
        """Expanded Europarl terminology (500+ terms)."""
        en_fr = {
            # Institutions
            # Legal terms
            # Political terms
            # Rights and freedoms
            # Economic terms
            # Policy areas
        }
        if direction == "fr-en":
            return {v: k for k, v in en_fr.items()}
        return en_fr
    
    def _get_expanded_scientific_glossary(self, direction="en-fr"):
        """Expanded scientific glossary (500+ terms across all domains)."""
        en_fr = {
            # Advanced ML/AI
            # Physics & Math advanced
            # Biology advanced
            # More common scientific terms
        }
        if direction == "fr-en":
            return {v: k for k, v in en_fr.items()}
        return en_fr
    
    def _get_wiktionary_terms(self, direction="en-fr"):
        """Common Wiktionary translation pairs."""
        en_fr = {
            "analysis": "analyse",
            "approach": "approche",
            "application": "application",
            "assessment": "√©valuation",
            "assumption": "hypoth√®se",
            "behavior": "comportement",
            "calculation": "calcul",
            "characteristic": "caract√©ristique",
            "comparison": "comparaison",
            "complexity": "complexit√©",
            "component": "composant",
            "concept": "concept",
            "condition": "condition",
            "configuration": "configuration",
            "constraint": "contrainte",
            "context": "contexte",
            "contribution": "contribution",
            "criterion": "crit√®re",
            "data": "donn√©es",
            "definition": "d√©finition",
            "description": "description",
            "development": "d√©veloppement",
            "dimension": "dimension",
            "distribution": "distribution",
            "effect": "effet",
            "efficiency": "efficacit√©",
            "element": "√©l√©ment",
            "environment": "environnement",
            "estimation": "estimation",
            "evaluation": "√©valuation",
            "evidence": "preuve",
            "example": "exemple",
            "experiment": "exp√©rience",
            "explanation": "explication",
            "expression": "expression",
            "factor": "facteur",
            "feature": "caract√©ristique",
            "framework": "cadre",
            "function": "fonction",
            "generation": "g√©n√©ration",
            "implementation": "impl√©mentation",
            "improvement": "am√©lioration",
            "indicator": "indicateur",
            "information": "information",
            "input": "entr√©e",
            "integration": "int√©gration",
            "interpretation": "interpr√©tation",
            "investigation": "investigation",
            "knowledge": "connaissance",
            "layer": "couche",
            "limitation": "limitation",
            "literature": "litt√©rature",
            "mechanism": "m√©canisme",
            "method": "m√©thode",
            "modification": "modification",
            "objective": "objectif",
            "observation": "observation",
            "operation": "op√©ration",
            "optimization": "optimisation",
            "output": "sortie",
            "parameter": "param√®tre",
            "performance": "performance",
            "perspective": "perspective",
            "phenomenon": "ph√©nom√®ne",
            "prediction": "pr√©diction",
            "principle": "principe",
            "probability": "probabilit√©",
            "problem": "probl√®me",
            "procedure": "proc√©dure",
            "process": "processus",
            "property": "propri√©t√©",
            "proposition": "proposition",
            "quality": "qualit√©",
            "quantity": "quantit√©",
            "range": "plage",
            "rate": "taux",
            "ratio": "rapport",
            "reduction": "r√©duction",
            "relation": "relation",
            "relationship": "relation",
            "representation": "repr√©sentation",
            "requirement": "exigence",
            "research": "recherche",
            "resource": "ressource",
            "response": "r√©ponse",
            "result": "r√©sultat",
            "review": "revue",
            "sample": "√©chantillon",
            "scale": "√©chelle",
            "scenario": "sc√©nario",
            "scope": "port√©e",
            "section": "section",
            "selection": "s√©lection",
            "sequence": "s√©quence",
            "series": "s√©rie",
            "set": "ensemble",
            "simulation": "simulation",
            "situation": "situation",
            "solution": "solution",
            "source": "source",
            "specification": "sp√©cification",
            "stability": "stabilit√©",
            "stage": "√©tape",
            "standard": "norme",
            "state": "√©tat",
            "statement": "d√©claration",
            "strategy": "strat√©gie",
            "structure": "structure",
            "study": "√©tude",
            "subject": "sujet",
            "summary": "r√©sum√©",
            "system": "syst√®me",
            "technique": "technique",
            "technology": "technologie",
            "term": "terme",
            "test": "test",
            "theory": "th√©orie",
            "threshold": "seuil",
            "tool": "outil",
            "training": "entra√Ænement",
            "transformation": "transformation",
            "trend": "tendance",
            "type": "type",
            "unit": "unit√©",
            "value": "valeur",
            "variable": "variable",
            "variation": "variation",
            "version": "version",
            "view": "vue",
        }
        if direction == "fr-en":
            return {v: k for k, v in en_fr.items()}
        return en_fr
    
    def _get_iate_terms(self, direction="en-fr"):
        """IATE (Inter-Active Terminology for Europe) terms."""
        en_fr = {
            # Technical/IT
            # Business
            # Environment
            # Healthcare
        }
        if direction == "fr-en":
            return {v: k for k, v in en_fr.items()}
        return en_fr
    
    # =========================================================================
    # Settings Functions
    # =========================================================================
    
    def save_api_key(self, backend, api_key):
        """Save API key."""
        if not api_key or not api_key.strip():
            return "Please enter an API key"
        
        env_map = {
            "openai": "OPENAI_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
            "deepseek": "DEEPSEEK_API_KEY",
        }
        
        env_var = env_map.get(backend, f"{backend.upper()}_API_KEY")
        os.environ[env_var] = api_key.strip()
        
        if "api_keys" not in self.config:
            self.config["api_keys"] = {}
        self.config["api_keys"][backend] = api_key.strip()
        self.save_config()
        
        return f"API key saved for {backend}"
    
    def get_api_keys_display(self):
        """Get masked API keys."""
        keys = self.config.get("api_keys", {})
        if not keys:
            return "No API keys configured"
        
        result = []
        for backend, key in keys.items():
            masked = key[:8] + "..." + key[-4:] if len(key) > 12 else "***"
            result.append(f"{backend}: {masked}")
        return "\n".join(result)
    
    def _get_api_keys_table(self):
        """Get API keys as table data for DataFrame."""
        keys = self.config.get("api_keys", {})
        env_map = {
            "openai": "OPENAI_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
            "deepseek": "DEEPSEEK_API_KEY",
            "huggingface": "HUGGINGFACE_API_KEY",
        }
        
        all_backends = ["openai", "anthropic", "deepseek", "huggingface"]
        table_data = []
        
        for backend in all_backends:
            env_var_name = env_map.get(backend)
            
            # Check config first (takes precedence)
            if backend in keys and keys[backend] and keys[backend].strip():
                masked = keys[backend][:8] + "..." + keys[backend][-4:] if len(keys[backend]) > 12 else "***"
                status = "‚úÖ Configured (Config)"
            # Then check environment variables
            elif env_var_name:
                env_key = os.environ.get(env_var_name)
                if env_key and env_key.strip() and len(env_key.strip()) > 0:
                    status = "‚úÖ From Environment"
                    # Show last 4 chars if key is long enough
                    if len(env_key.strip()) > 4:
                        masked = "***" + env_key.strip()[-4:]
                    else:
                        masked = "***"
                else:
                    status = "‚ùå Not Set"
                    masked = "-"
            else:
                status = "‚ùå Not Set"
                masked = "-"
            
            table_data.append([backend.capitalize(), status, masked])
        
        return table_data
    
    def delete_api_key(self, backend):
        """Delete API key for a backend."""
        if "api_keys" not in self.config:
            self.config["api_keys"] = {}
        
        if backend in self.config["api_keys"]:
            del self.config["api_keys"][backend]
            self.save_config()
            
            # Also remove from environment
            env_map = {
                "openai": "OPENAI_API_KEY",
                "anthropic": "ANTHROPIC_API_KEY",
                "deepseek": "DEEPSEEK_API_KEY",
                "huggingface": "HUGGINGFACE_API_KEY",
            }
            env_var = env_map.get(backend)
            if env_var and env_var in os.environ:
                del os.environ[env_var]
            
            return f"API key deleted for {backend}"
        else:
            return f"No API key found for {backend}"
    
    def save_all_settings(
        self, backend, masking, reranking, cache, glossary, context, 
        context_window, candidates, strict_mode, fallback
    ):
        """Save all settings."""
        self.config["default_backend"] = backend
        self.config["masking_enabled"] = masking
        self.config["reranking_enabled"] = reranking
        self.config["cache_enabled"] = cache
        self.config["glossary_enabled"] = glossary
        self.config["context_enabled"] = context
        self.config["context_window"] = int(context_window) if context_window else 5
        self.config["max_candidates"] = int(candidates) if candidates else 3
        self.config["strict_mode"] = strict_mode
        self.config["enable_fallback"] = fallback
        self.save_config()
        return "‚úÖ All settings saved successfully!"
    
    def reset_settings(self):
        """Reset settings to defaults."""
        self.config = self._default_config()
        self.save_config()
        return "‚úÖ Settings reset to defaults"
    
    def clear_cache(self):
        """Clear translation cache."""
        try:
            import shutil
            cache_dir = Path(".cache/translations")
            if cache_dir.exists():
                shutil.rmtree(cache_dir)
                cache_dir.mkdir(parents=True)
            return "Cache cleared"
        except Exception as e:
            return f"Error: {str(e)}"
    
    # =========================================================================
    # Create Interface
    # =========================================================================
    
    def create_interface(self):
        """Create the Gradio interface."""
        
        with gr.Blocks(title="SciTrans LLMs") as demo:
            
            # Header
            gr.Markdown("# üî¨ SciTrans LLMs\n**Scientific Document Translation** ‚Ä¢ v1.0")
            
            with gr.Tabs():
                
                # ===========================================================
                # TAB 1: TRANSLATION
                # ===========================================================
                with gr.Tab("Translation"):
                    allowed_backends = ["deepseek", "openai", "anthropic", "cascade", "free", "ollama", "local", "libre", "argos"]
                    initial_backend = self.config.get("default_backend", "deepseek")
                    if initial_backend not in allowed_backends:
                        initial_backend = "deepseek"
                    initial_model_opts = self._get_model_options_for_backend(initial_backend)
                    # Ensure initial model value is always valid (never "default" for backends that don't support it)
                    if initial_backend in ["ollama", "huggingface", "openai", "anthropic", "deepseek"]:
                        # These backends don't support "default", so ensure we use a valid model
                        if initial_model_opts["value"] == "default" or initial_model_opts["value"] not in initial_model_opts["choices"]:
                            initial_model_opts["value"] = initial_model_opts["choices"][0] if initial_model_opts["choices"] else "default"

                    with gr.Row():
                        # Left: Controls (narrower)
                        with gr.Column(scale=2):
                            # Input method tabs: Upload or URL
                            with gr.Tabs():
                                with gr.Tab("üìÅ Upload PDF"):
                                    pdf_upload = gr.File(label="Upload PDF", file_types=[".pdf"])
                                with gr.Tab("üîó From URL"):
                                    pdf_url = gr.Textbox(
                                        label="PDF URL",
                                        placeholder="Enter URL to PDF (e.g., https://arxiv.org/pdf/1234.5678.pdf)",
                                        lines=1
                                    )
                                    url_load_btn = gr.Button("üì• Load PDF from URL", variant="primary")
                                    url_status = gr.Markdown("", visible=False)
                            
                            with gr.Row():
                                source_lang = gr.Dropdown(["en", "fr"], value="en", label="From", scale=1)
                                target_lang = gr.Dropdown(["fr", "en"], value="fr", label="To", scale=1)
                            
                            backend = gr.Dropdown(
                                allowed_backends,
                                value=initial_backend,
                                label="Backend"
                            )
                            
                            model_selector = gr.Dropdown(
                                initial_model_opts["choices"],
                                value=initial_model_opts["value"],
                                label="Model (for Ollama/HuggingFace/OpenAI/etc.)",
                                info="Select model for backends that support it",
                                visible=initial_model_opts["visible"]
                            )
                            
                            # All advanced features ON by default
                            advanced_options = gr.CheckboxGroup(
                                ["Masking", "Reranking", "Context", "Glossary"],
                                value=["Masking", "Reranking", "Context", "Glossary"],
                                label="Advanced Features"
                            )
                            
                            # Advanced tweakable parameters
                            with gr.Accordion("‚öôÔ∏è Advanced Parameters", open=False):
                                num_candidates = gr.Slider(
                                    minimum=1,
                                    maximum=4,
                                    step=1,
                                    value=2,
                                    label="Number of candidates (turns)"
                                )
                                context_window = gr.Slider(
                                    minimum=0,
                                    maximum=10,
                                    step=1,
                                    value=5,
                                    label="Context window (blocks)"
                                )
                                prompt_rounds = gr.Slider(
                                    minimum=0,
                                    maximum=5,
                                    step=1,
                                    value=2,
                                    label="Prompt optimization rounds"
                                )
                                quality_threshold = gr.Slider(
                                    minimum=0.0,
                                    maximum=1.0,
                                    step=0.05,
                                    value=0.7,
                                    label="Quality threshold"
                                )
                                batch_size = gr.Slider(
                                    minimum=5,
                                    maximum=30,
                                    step=1,
                                    value=10,
                                    label="Batch size",
                                    info="Number of blocks to process in parallel"
                                )
                                enable_parallel = gr.Checkbox(
                                    value=True,
                                    label="Enable Parallel Processing",
                                    info="Use parallel processing for large documents (faster)"
                                )
                                max_workers = gr.Number(
                                    value=None,
                                    precision=0,
                                    label="Max Workers (optional)",
                                    info="Max parallel workers (leave blank for auto-detect)"
                                )
                                adaptive_concurrency = gr.Checkbox(
                                    value=True,
                                    label="Adaptive Concurrency",
                                    info="Automatically adjust concurrency based on backend and document size"
                                )
                                with gr.Row():
                                    start_page = gr.Number(
                                        value=0,
                                        precision=0,
                                        label="Start page (0-based, inclusive)"
                                    )
                                    end_page = gr.Number(
                                        value=None,
                                        precision=0,
                                        label="End page (0-based, inclusive; blank = all)"
                                    )
                                font_dir = gr.Textbox(
                                    value="",
                                    label="Font directory (TTF/OTF) for embedding (optional)",
                                    placeholder="/path/to/fonts"
                                )
                                font_files = gr.Textbox(
                                    value="",
                                    label="Font files (comma-separated, optional; overrides directory priority)",
                                    placeholder="/path/to/font1.ttf,/path/to/font2.otf"
                                )
                                font_priority = gr.Textbox(
                                    value="",
                                    label="Font priority keywords (comma-separated, optional)",
                                    placeholder="roboto,helvetica"
                                )
                                mask_custom_macros = gr.Checkbox(
                                    value=True,
                                    label="Mask custom LaTeX macros (newcommand/DeclareMathOperator/etc.)"
                                )
                                mask_apostrophes_in_latex = gr.Checkbox(
                                    value=True,
                                    label="Protect apostrophes inside LaTeX/math"
                                )
                            
                            with gr.Row():
                                translate_btn = gr.Button("üöÄ Translate", variant="primary")
                                retranslate_btn = gr.Button("üîÅ Retranslate", variant="secondary")
                                clear_btn = gr.Button("üßπ Clear", variant="secondary")
                            
                            # Status/Log moved to left column below translate button
                            with gr.Accordion("üìä Status & Logs", open=False):
                                with gr.Tabs():
                                    with gr.Tab("Status"):
                                        status_box = gr.Textbox(
                                            lines=4, 
                                            interactive=False, 
                                            show_label=False,
                                            placeholder="Status will appear here..."
                                        )
                                    with gr.Tab("Log"):
                                        log_box = gr.Textbox(
                                            lines=20, 
                                            interactive=False, 
                                            show_label=False, 
                                            autoscroll=True,
                                            placeholder="Translation logs will appear here..."
                                        )
                                    with gr.Tab("Performance"):
                                        perf_info = gr.Textbox(
                                            lines=4,
                                            interactive=False,
                                            show_label=False,
                                            placeholder="Performance metrics will appear here..."
                                        )
                                    with gr.Tab("Quality Scores"):
                                        score_info = gr.Textbox(
                                            lines=15,
                                            interactive=False,
                                            show_label=False,
                                            placeholder="Translation quality scores will appear here after translation..."
                                        )
                        
                        # Right: Preview (wider) - Preview replaces progress/status area
                        with gr.Column(scale=3):
                            # Download button (small footprint)
                            download_btn = gr.DownloadButton(
                                label="üì• Download Translated PDF",
                                value=None,
                                interactive=False,
                                visible=False
                            )
                            
                            # Preview area with PDF rendering to images
                            # STEP 8: Render PDFs to images for proper preview
                            with gr.Tabs(selected=0):  # selected=0 prevents auto-switching
                                with gr.Tab("Source"):
                                    source_preview = gr.Image(
                                        label="Source PDF Preview",
                                        height=600,
                                        show_label=False,
                                        container=False,
                                        type="numpy"
                                    )
                                with gr.Tab("Translated"):
                                    trans_preview = gr.Image(
                                        label="Translated PDF Preview",
                                        height=600,
                                        show_label=False,
                                        container=False,
                                        type="numpy"
                                    )
                                with gr.Tab("Text Preview"):
                                    translation_preview = gr.Textbox(
                                        lines=20,
                                        label="Translation Preview (Before Rendering)",
                                        interactive=False,
                                        placeholder="Translation preview will appear here after translation completes..."
                                    )
                            
                            # Loading indicator (shown during translation)
                            loading_indicator = gr.Markdown("", visible=False)
                            
                            # Unified pagination
                            gr.Markdown("**Pages**")
                            with gr.Row():
                                page_prev = gr.Button("‚óÄ", size="sm", scale=0, min_width=30)
                                page_slider = gr.Slider(
                                    minimum=1,
                                    maximum=1,
                                    step=1,
                                    value=1,
                                    label="Page",
                                    interactive=True
                                )
                                page_next = gr.Button("‚ñ∂", size="sm", scale=0, min_width=30)
                                page_total = gr.Textbox(value="of 1", show_label=False, interactive=False, scale=0, min_width=60, container=False)
                
                # ===========================================================
                # TAB 2: TESTING
                # ===========================================================
                with gr.Tab("Testing"):
                    gr.Markdown("### üß™ Component Tests\nTest individual components before full translation.")
                    
                    with gr.Row():
                        with gr.Column():
                            gr.Markdown("**üîå Backend Test**")
                            test_backend_sel = gr.Dropdown(
                                ["deepseek", "openai", "anthropic", "cascade", "free", "ollama", "local", "libre", "argos", "huggingface"],
                                value="deepseek", label="Backend",
                                info="DeepSeek recommended. Free options: cascade, free, local, libre, argos"
                            )
                            # Pre-filled with rich test content
                            test_text = gr.Textbox(
                                value="""# Machine Learning Overview

Machine learning enables computers to learn from data without explicit programming.

## Key Concepts:
‚Ä¢ **Supervised Learning** - Learn from labeled examples
‚Ä¢ **Unsupervised Learning** - Find patterns in unlabeled data
‚Ä¢ **Reinforcement Learning** - Learn through rewards/penalties

The loss function $L(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$ measures prediction error.

See: https://arxiv.org/abs/1234.5678 for more details.""",
                                label="Sample Text (includes headers, bullets, math, URLs)", lines=8
                            )
                            test_backend_btn = gr.Button("‚ñ∂ Test Backend")
                            test_backend_result = gr.Textbox(label="Result", lines=6)
                        
                        with gr.Column():
                            gr.Markdown("**üé≠ Masking Test**")
                            # Rich test content with various maskable elements
                            masking_input = gr.Textbox(
                                value="""The famous equation $E=mc^2$ demonstrates mass-energy equivalence.

For more complex formulas, consider:
$$\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}$$

References:
‚Ä¢ Einstein, A. (1905) - https://doi.org/10.1002/andp.19053221004
‚Ä¢ Code example: `import numpy as np`
‚Ä¢ Email: researcher@university.edu
‚Ä¢ LaTeX block: \\begin{equation}F = ma\\end{equation}""",
                                label="Test Input (LaTeX, URLs, code, emails)", lines=8
                            )
                            test_masking_btn = gr.Button("‚ñ∂ Test Masking")
                            test_masking_result = gr.Textbox(label="Result", lines=6)
                    
                    with gr.Row():
                        with gr.Column():
                            gr.Markdown("**üìÑ Layout Test**\nTest font detection, headers, footers, structure.")
                            layout_pdf = gr.File(label="Upload PDF to analyze layout", file_types=[".pdf"])
                            test_layout_btn = gr.Button("‚ñ∂ Test Layout")
                            test_layout_result = gr.Textbox(label="Result", lines=15)
                        
                        with gr.Column():
                            gr.Markdown("**üíæ Cache Test**\nVerify translation caching is working.")
                            test_cache_btn = gr.Button("‚ñ∂ Test Cache")
                            test_cache_result = gr.Textbox(label="Result", lines=6)
                    
                    # Additional test samples
                    with gr.Accordion("üìù More Test Samples", open=False):
                        gr.Markdown("""
### Font & Style Tests
Copy these to test different formatting:

**Title Style:**
`NEURAL NETWORK ARCHITECTURES FOR SCIENTIFIC DOCUMENT ANALYSIS`

**Abstract Style:**
`Abstract: This paper presents a novel approach to machine translation using transformer architectures with attention mechanisms.`

**Section Headers:**
`1. Introduction`
`2.1 Related Work`
`3.2.1 Methodology Details`

**Bullet Points:**
`‚Ä¢ First item with **bold** text`
`‚Ä¢ Second item with *italic* text`
`‚Ä¢ Third item with code: model.fit(X, y)`

**Numbered List:**
`1. First step in the process`
`2. Second step with equation $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$`
`3. Third step referencing [Author2023]`

**Footer/Header:**
`Page 1 of 10 | Confidential Draft | DOI: 10.1234/example.2024`
                        """)
                
                # ===========================================================
                # TAB 3: SETTINGS
                # ===========================================================
                with gr.Tab("Settings"):
                    with gr.Row():
                        with gr.Column(scale=1):
                            gr.Markdown("### üîë API Keys Management")
                            
                            # API Keys Table/List
                            api_keys_table = gr.Dataframe(
                                headers=["Backend", "Status", "Key Preview"],
                                value=self._get_api_keys_table(),
                                label="Configured API Keys",
                                interactive=False,
                                wrap=True
                            )
                            
                            refresh_keys_btn = gr.Button("üîÑ Refresh Keys Table", variant="secondary", size="sm")
                            
                            gr.Markdown("**Add/Update API Key**")
                            api_backend = gr.Dropdown(
                                ["openai", "anthropic", "deepseek", "huggingface"],
                                value="openai", 
                                label="Backend",
                                info="Select backend to configure"
                            )
                            api_key_input = gr.Textbox(
                                label="API Key", 
                                type="password",
                                placeholder="Enter API key here..."
                            )
                            with gr.Row():
                                save_key_btn = gr.Button("üíæ Save Key", variant="primary")
                                delete_key_btn = gr.Button("üóëÔ∏è Delete Key", variant="stop")
                            
                            api_status = gr.Textbox(label="Status", lines=2, interactive=False)
                            
                            # Refresh button for API keys table
                            refresh_keys_btn = gr.Button("üîÑ Refresh Keys Table", variant="secondary", size="sm")
                            
                            gr.Markdown("### üìã Environment Variables")
                            gr.Markdown("""
                            You can also set API keys via environment variables:
                            - `OPENAI_API_KEY` for OpenAI
                            - `ANTHROPIC_API_KEY` for Anthropic
                            - `DEEPSEEK_API_KEY` for DeepSeek
                            - `HUGGINGFACE_API_KEY` for HuggingFace
                            
                            Keys set here override environment variables.
                            """)
                        
                        with gr.Column(scale=1):
                            gr.Markdown("### ‚öôÔ∏è Translation Settings")
                            
                            set_backend = gr.Dropdown(
                                ["deepseek", "openai", "anthropic", "cascade", "free", "ollama", "local", "libre", "argos", "huggingface"],
                                value=self.config.get("default_backend", "deepseek"),
                                label="Default Backend",
                                info="DeepSeek recommended for best quality/price. Free options have rate limits."
                            )
                            
                            with gr.Accordion("üîß Core Features", open=True):
                                set_masking = gr.Checkbox(
                                    value=self.config.get("masking_enabled", True), 
                                    label="Enable Masking",
                                    info="Protect LaTeX, URLs, code from translation"
                                )
                                set_reranking = gr.Checkbox(
                                    value=self.config.get("reranking_enabled", True), 
                                    label="Enable Reranking",
                                    info="Select best translation from multiple candidates"
                                )
                                set_cache = gr.Checkbox(
                                    value=self.config.get("cache_enabled", True), 
                                    label="Enable Cache",
                                    info="Cache translations for faster re-runs"
                                )
                                set_glossary = gr.Checkbox(
                                    value=self.config.get("glossary_enabled", True), 
                                    label="Enable Glossary",
                                    info="Use domain-specific terminology"
                                )
                                set_context = gr.Checkbox(
                                    value=self.config.get("context_enabled", True),
                                    label="Enable Document Context",
                                    info="Maintain consistency across document"
                                )
                            
                            with gr.Accordion("üìä Advanced Parameters", open=False):
                                set_context_window = gr.Slider(
                                    0, 10, 
                                    value=self.config.get("context_window", 5), 
                                    step=1, 
                                    label="Context Window Size",
                                    info="Number of previous blocks to consider"
                                )
                                set_candidates = gr.Slider(
                                    1, 5, 
                                    value=self.config.get("max_candidates", 3), 
                                    step=1, 
                                    label="Max Translation Candidates",
                                    info="Number of candidates for reranking"
                                )
                                set_strict_mode = gr.Checkbox(
                                    value=self.config.get("strict_mode", True),
                                    label="Strict Mode",
                                    info="Fail loudly if translation incomplete"
                                )
                                set_fallback = gr.Checkbox(
                                    value=self.config.get("enable_fallback", True),
                                    label="Enable Fallback Backend",
                                    info="Use stronger backend if primary fails"
                                )
                            
                            save_settings_btn = gr.Button("üíæ Save All Settings", variant="primary")
                            settings_status = gr.Textbox(label="", interactive=False)
                            
                            gr.Markdown("### üõ†Ô∏è Maintenance")
                            with gr.Row():
                                clear_cache_btn = gr.Button("üóëÔ∏è Clear Cache", variant="secondary")
                                reset_settings_btn = gr.Button("üîÑ Reset to Defaults", variant="secondary")
                            cache_status = gr.Textbox(label="", interactive=False)
                            
                            gr.Markdown("### üìù Basic Commands")
                            gr.Markdown("""
                            **CLI Commands:**
                            ```bash
                            # Basic translation
                            scitrans translate input.pdf -o output.pdf
                            
                            # With backend
                            scitrans translate input.pdf --backend openai
                            
                            # With strict mode
                            scitrans translate input.pdf --strict-mode true
                            
                            # List backends
                            scitrans backends
                            
                            # Run tests
                            scitrans test
                            ```
                            """)
                
                # ===========================================================
                # TAB 4: GLOSSARY
                # ===========================================================
                with gr.Tab("Glossary"):
                    gr.Markdown("### üìö Domain Terminology Management")
                    
                    # Explanation box
                    with gr.Accordion("‚ÑπÔ∏è How Glossary Works", open=False):
                        gr.Markdown("""
**What is the Glossary?**
The glossary is a dictionary of domain-specific terms that ensures consistent, accurate translations of technical vocabulary.

**How it's used:**
1. **During Translation**: When the system encounters a term in the glossary, it uses your specified translation instead of the generic one
2. **Pattern Matching**: Terms are matched case-insensitively in the source text
3. **Priority**: Glossary translations take precedence over backend translations

**Where is it stored?**
- **Session Glossary**: Loaded terms stay in memory during your session
- **Persistent Storage**: All loaded glossaries are saved to `~/.scitrans/glossary.json` and auto-load on next launch
- **Cache Integration**: Glossary-enhanced translations are cached for speed

**Best Practices:**
- Load domain-specific glossaries matching your document type
- Add custom terms for unique terminology in your field
- Use "Load ALL" for comprehensive coverage across domains
                        """)
                    
                    with gr.Row():
                        with gr.Column(scale=1):
                            # Domain selector dropdown
                            gr.Markdown("**üî¨ Built-in Scientific Glossaries**")
                            glossary_domain = gr.Dropdown(
                                choices=[
                                    ("ü§ñ Machine Learning & AI (50+ terms)", "ml"),
                                    ("‚öõÔ∏è Physics & Mathematics (40+ terms)", "physics"),
                                    ("üß¨ Biology & Medical (40+ terms)", "biology"),
                                    ("üèõÔ∏è Legal & EU Institutions (20+ terms)", "europarl"),
                                    ("üî¨ Chemistry (30+ terms)", "chemistry"),
                                    ("üíª Computer Science (40+ terms)", "cs"),
                                    ("üìä Statistics (25+ terms)", "statistics"),
                                ],
                                label="Select Domain",
                                value="ml"
                            )
                            glossary_direction = gr.Radio(
                                choices=["EN ‚Üí FR", "FR ‚Üí EN"],
                                value="EN ‚Üí FR",
                                label="Direction"
                            )
                            with gr.Row():
                                load_domain_btn = gr.Button("üì• Load Selected", variant="primary")
                                load_all_btn = gr.Button("üìö Load ALL Built-in (250+ terms)", variant="secondary")
                            
                            gr.Markdown("---")
                            gr.Markdown("**üåê Online Glossary Sources**")
                            online_source = gr.Dropdown(
                                choices=[
                                    ("üá™üá∫ Europarl Extended (100+ EU/Legal terms)", "europarl_full"),
                                    ("üî¨ Scientific Extended (150+ research terms)", "huggingface_opus"),
                                    ("üìñ Wiktionary Common Terms (100+ general)", "wiktionary"),
                                    ("üèõÔ∏è IATE EU Terminology (70+ official terms)", "iate"),
                                ],
                                label="Select Online Source",
                                value="europarl_full"
                            )
                            load_online_btn = gr.Button("üåê Load from Online", variant="secondary")
                            
                            gr.Markdown("---")
                            gr.Markdown("**üìÅ Custom Glossary**")
                            glossary_file = gr.File(label="Upload JSON file", file_types=[".json"])
                            load_file_btn = gr.Button("üì• Load from File", size="sm")
                            
                            gr.Markdown("---")
                            gr.Markdown("**‚úèÔ∏è Add Individual Term**")
                            with gr.Row():
                                term_source = gr.Textbox(label="Source", placeholder="neural network", scale=1)
                                term_target = gr.Textbox(label="Translation", placeholder="r√©seau de neurones", scale=1)
                            with gr.Row():
                                add_term_btn = gr.Button("‚ûï Add", size="sm", variant="primary")
                                clear_gloss_btn = gr.Button("üóëÔ∏è Clear All", size="sm", variant="stop")
                            
                            glossary_status = gr.Textbox(label="Status", lines=2, interactive=False, value=f"Glossary loaded: {len(self.glossary)} terms\nLocation: ~/.scitrans/glossary.json")
                        
                        with gr.Column(scale=1):
                            gr.Markdown("**üìñ Current Glossary**")
                            term_count = gr.Textbox(value=f"{len(self.glossary)} terms loaded", label="", interactive=False)
                            glossary_preview = gr.Dataframe(
                                headers=["Source Term", "Translation"],
                                value=[[k, v] for k, v in list(self.glossary.items())[:50]] if self.glossary else [],
                                label="Terms (showing first 50)",
                                wrap=True,
                                interactive=False
                            )
                            
                            gr.Markdown("""
**Available Glossary Sources:**

üîπ **Built-in** (offline, instant):
- ML/AI, Physics, Biology, Chemistry, CS, Statistics, Legal

üîπ **Online** (fetched from web):
- Europarl: Official EU translation terminology
- HuggingFace OPUS: Multilingual parallel corpus terms
- Wiktionary: Common academic vocabulary
- IATE: Inter-Active Terminology for Europe

**JSON Format for Custom Upload:**
```json
{
  "source term": "translated term",
  "neural network": "r√©seau de neurones"
}
```
                            """)
                
                # ===========================================================
                # TAB 5: ABOUT
                # ===========================================================
                with gr.Tab("About"):
                    with gr.Row():
                        with gr.Column():
                            gr.Markdown("""
                            ## SciTrans LLMs v1.0
                            
                            Scientific Document Translation System with layout preservation.
                            
                            ### Features
                            - **Masking**: Protects LaTeX, URLs, DOIs, code blocks
                            - **Reranking**: Multi-candidate translation selection
                            - **Context**: Document-level consistency
                            - **Layout**: Preserves PDF structure and fonts
                            - **Caching**: Persistent cache for faster re-translations
                            
                            ### Backends
                            | Backend | Type | Cost |
                            |---------|------|------|
                            | cascade | Free | Free |
                            | free | Free | Free |
                            | local | Local | Free |
                            | libre | Free | Free |
                            | argos | Local | Free |
                            | ollama | Local | Free |
                            | openai | API | $$$ |
                            | anthropic | API | $$$ |
                            | deepseek | API | $ |
                            | huggingface | API/Free | Free/$$$ |
                            """)
                        
                        with gr.Column():
                            gr.Markdown("""
                            ### CLI Usage
                            
                            ```bash
                            # Basic translation
                            ./scitrans.sh translate paper.pdf -o output.pdf
                            
                            # With specific backend
                            ./scitrans.sh translate paper.pdf --backend openai
                            
                            # Run tests
                            ./scitrans.sh test
                            
                            # List backends
                            ./scitrans.sh backends
                            ```
                            
                            ### Tips
                            - Use **Cascade** for free translations
                            - Enable **Reranking** for academic papers
                            - Add domain terms to **Glossary** tab
                            - Use **Context** for long documents
                            - Cache persists between sessions for speed
                            """)
            
            # ===========================================================
            # EVENT HANDLERS
            # ===========================================================
            
            # PDF upload
            def on_upload(pdf):
                self.translated_pdf_path = None
                if pdf is None:
                    self.source_pdf_path = None
                    return None, None, gr.update(maximum=1, value=1), "of 1", gr.update(visible=False, value="")
                # Store source PDF path for preview (ensure absolute path)
                pdf_path = pdf.name if hasattr(pdf, 'name') else str(pdf)
                pdf_path = str(Path(pdf_path).resolve())  # Convert to absolute path
                self.source_pdf_path = pdf_path
                count = self.get_page_count(pdf)
                # Render first page as image for preview
                source_img = self.render_pdf_page(pdf_path, 0)
                # Return source PDF image, None for translated (not translated yet)
                return source_img, None, gr.update(maximum=max(1, count), value=1), f"of {max(1, count)}", gr.update(visible=False, value="")
            
            # URL download
            def on_url_load(url):
                if not url or not url.strip():
                    self.source_pdf_path = None
                    return None, None, gr.update(maximum=1, value=1), "of 1", gr.update(visible=True, value="‚ùå Please enter a URL")
                
                pdf_path, status_msg = self.download_pdf_from_url(url)
                if pdf_path:
                    # Store source PDF path (ensure absolute path)
                    pdf_path = str(Path(pdf_path).resolve())  # Convert to absolute path
                    self.source_pdf_path = pdf_path
                    # Get page count
                    count = self.get_page_count(type('obj', (object,), {'name': pdf_path})())
                    # Render first page as image
                    source_img = self.render_pdf_page(pdf_path, 0)
                    # Return source PDF image, None for translated
                    return source_img, None, gr.update(maximum=max(1, count), value=1), f"of {max(1, count)}", gr.update(visible=True, value=status_msg)
                else:
                    self.source_pdf_path = None
                    return None, None, gr.update(maximum=1, value=1), "of 1", gr.update(visible=True, value=status_msg)
            
            pdf_upload.change(fn=on_upload, inputs=[pdf_upload], outputs=[source_preview, trans_preview, page_slider, page_total, url_status])
            url_load_btn.click(fn=on_url_load, inputs=[pdf_url], outputs=[source_preview, trans_preview, page_slider, page_total, url_status])
            
            # Unified page navigation for source and translated previews
            # Update both source and translated previews independently based on page slider
            def nav_page(pdf, page, total, direction):
                # Get source PDF path (from upload or stored) - ensure absolute path
                source_path = self.source_pdf_path if self.source_pdf_path else (pdf.name if hasattr(pdf, 'name') else str(pdf) if pdf else None)
                if source_path:
                    source_path = str(Path(source_path).resolve())
                # Get translated PDF path (only available after translation) - ensure absolute path
                trans_path = self.translated_pdf_path if getattr(self, "translated_pdf_path", None) else None
                if trans_path:
                    trans_path = str(Path(trans_path).resolve())
                
                try:
                    max_p = int(str(total).replace("of", "").strip())
                except Exception:
                    max_p = 1
                new_p = max(1, min(max_p, int(page) + direction))
                
                # Render PDF pages as images (0-indexed, so subtract 1)
                source_img = self.render_pdf_page(source_path, new_p - 1) if source_path else None
                trans_img = self.render_pdf_page(trans_path, new_p - 1) if trans_path else None
                
                # Return source PDF image, translated PDF image, and new page number
                return source_img, trans_img, new_p
            
            page_prev.click(
                fn=lambda p, pg, t: nav_page(p, pg, t, -1),
                inputs=[pdf_upload, page_slider, page_total],
                outputs=[source_preview, trans_preview, page_slider],
            )
            page_next.click(
                fn=lambda p, pg, t: nav_page(p, pg, t, 1),
                inputs=[pdf_upload, page_slider, page_total],
                outputs=[source_preview, trans_preview, page_slider],
            )
            page_slider.change(
                fn=lambda p, pg, t: nav_page(p, pg, t, 0),
                inputs=[pdf_upload, page_slider, page_total],
                outputs=[source_preview, trans_preview, page_slider],
            )
            
            # Translation
            translate_inputs = [
                pdf_upload,
                source_lang,
                target_lang,
                backend,
                model_selector,
                advanced_options,
                num_candidates,
                context_window,
                quality_threshold,
                prompt_rounds,
                batch_size,
                enable_parallel,
                max_workers,
                adaptive_concurrency,
                start_page,
                end_page,
                font_dir,
                font_files,
                font_priority,
                mask_custom_macros,
                mask_apostrophes_in_latex,
            ]
            translate_outputs = [status_box, download_btn, log_box, trans_preview, page_total, page_slider, source_preview, perf_info, translation_preview, score_info]
            
            # Wrapper to add error handling and logging
            def translate_wrapper(*args):
                """Wrapper to catch and log any errors during translation."""
                try:
                    print(f"[GUI DEBUG] translate_wrapper called with {len(args)} arguments")
                    result = self.translate_document(*args)
                    print(f"[GUI DEBUG] translate_document returned successfully")
                    return result
                except Exception as e:
                    print(f"[GUI DEBUG] ERROR in translate_wrapper: {e}")
                    import traceback
                    traceback.print_exc()
                    # Return error state
                    error_msg = f"‚ùå Translation failed: {str(e)}\n\nCheck console/logs for details."
                    return (
                        error_msg,
                        gr.update(value=None, visible=False),
                        error_msg,
                        None,
                        "of 0",
                        gr.update(maximum=1, value=1),
                        None,
                        f"Error: {str(e)}",
                        f"Error: {str(e)}",
                        "No scoring data available due to error."
                    )
            
            translate_btn.click(
                fn=translate_wrapper,
                inputs=translate_inputs,
                outputs=translate_outputs
            )
            retranslate_btn.click(
                fn=translate_wrapper,
                inputs=translate_inputs,
                outputs=translate_outputs
            )
            
            def clear_all():
                self.translated_pdf_path = None
                self.source_pdf_path = None
                return "", gr.update(value=None, visible=False), "", None, "of 1", gr.update(maximum=1, value=1), None, "", "", ""  # None for images, empty score
            
            clear_btn.click(fn=clear_all, outputs=translate_outputs)
            
            def update_backend_and_model(backend_value):
                """Update model options when backend changes, ensuring valid value."""
                opts = self._get_model_options_for_backend(backend_value)
                # Always reset to first valid option to prevent invalid "default" values
                return gr.update(choices=opts["choices"], value=opts["value"], visible=opts["visible"])
            
            backend.change(
                fn=update_backend_and_model,
                inputs=[backend],
                outputs=[model_selector]
            )
            
            # Testing
            test_backend_btn.click(fn=self.test_backend, inputs=[test_backend_sel, test_text], outputs=[test_backend_result])
            test_masking_btn.click(fn=self.test_masking, inputs=[masking_input], outputs=[test_masking_result])
            test_layout_btn.click(fn=self.test_layout, inputs=[layout_pdf], outputs=[test_layout_result])
            test_cache_btn.click(fn=self.test_cache, outputs=[test_cache_result])
            
            # Settings
            def update_api_keys_table():
                return self._get_api_keys_table()
            
            refresh_keys_btn.click(
                fn=update_api_keys_table,
                outputs=[api_keys_table]
            )
            
            save_key_btn.click(
                fn=self.save_api_key, 
                inputs=[api_backend, api_key_input], 
                outputs=[api_status]
            ).then(
                fn=update_api_keys_table, 
                outputs=[api_keys_table]
            )
            
            delete_key_btn.click(
                fn=self.delete_api_key,
                inputs=[api_backend],
                outputs=[api_status]
            ).then(
                fn=update_api_keys_table,
                outputs=[api_keys_table]
            )
            
            save_settings_btn.click(
                fn=self.save_all_settings,
                inputs=[
                    set_backend, set_masking, set_reranking, set_cache, 
                    set_glossary, set_context, set_context_window, 
                    set_candidates, set_strict_mode, set_fallback
                ],
                outputs=[settings_status]
            )
            
            reset_settings_btn.click(
                fn=self.reset_settings,
                outputs=[settings_status]
            )
            
            clear_cache_btn.click(fn=self.clear_cache, outputs=[cache_status])
            
            # Glossary - new dropdown-based UI
            def load_selected_domain(domain, direction):
                dir_code = "en-fr" if direction == "EN ‚Üí FR" else "fr-en"
                return self.load_glossary_domain(domain, dir_code)
            
            def get_glossary_preview():
                # Return first 50 terms for preview as DataFrame format
                if self.glossary:
                    return [[k, v] for k, v in list(self.glossary.items())[:50]]
                return []
            
            load_domain_btn.click(
                fn=load_selected_domain,
                inputs=[glossary_domain, glossary_direction],
                outputs=[glossary_status, glossary_preview]
            ).then(fn=lambda: f"{len(self.glossary)} terms loaded", outputs=[term_count])
            
            def load_all_with_direction(direction):
                dir_code = "en-fr" if direction == "EN ‚Üí FR" else "fr-en"
                return self.load_all_scientific_glossaries(dir_code)
            
            load_all_btn.click(
                fn=load_all_with_direction,
                inputs=[glossary_direction],
                outputs=[glossary_status, glossary_preview]
            ).then(fn=lambda: f"{len(self.glossary)} terms loaded", outputs=[term_count])
            
            # File upload and manual terms
            load_file_btn.click(
                fn=self.load_glossary_file, 
                inputs=[glossary_file], 
                outputs=[glossary_status, glossary_preview]
            ).then(fn=lambda: f"{len(self.glossary)} terms loaded", outputs=[term_count])
            
            add_term_btn.click(
                fn=self.add_glossary_term, 
                inputs=[term_source, term_target], 
                outputs=[glossary_status, glossary_preview]
            ).then(fn=lambda: f"{len(self.glossary)} terms", outputs=[term_count])
            
            clear_gloss_btn.click(
                fn=self.clear_glossary, 
                outputs=[glossary_status, glossary_preview]
            ).then(fn=lambda: "0 terms", outputs=[term_count])
            
            # Online glossary loading
            def load_online_glossary_handler(source, direction):
                dir_code = "en-fr" if direction == "EN ‚Üí FR" else "fr-en"
                return self.load_online_glossary(source, dir_code)
            
            load_online_btn.click(
                fn=load_online_glossary_handler,
                inputs=[online_source, glossary_direction],
                outputs=[glossary_status, glossary_preview]
            ).then(fn=lambda: f"{len(self.glossary)} terms loaded", outputs=[term_count])
        
        return demo


def find_free_port(start_port=7860, max_attempts=10):
    """Find a free port starting from start_port."""
    import socket
    for port in range(start_port, start_port + max_attempts):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(('', port))
                return port
        except OSError:
            continue
    # If no free port found in range, return None to let Gradio handle it
    return None


def launch(share=False, port=None):
    """Launch the GUI."""
    if port is None:
        port = find_free_port(7860)
        if port is None:
            # If no free port found, let Gradio find one automatically
            port = 0  # 0 means "let OS assign a free port"
    
    # Build launch kwargs
    launch_kwargs = {
        "server_name": "0.0.0.0",
        "share": share,  # Explicitly set share=False to suppress Gradio warning
        "inbrowser": True,
        "show_error": True,
        "allowed_paths": [str(Path(tempfile.gettempdir())), str(Path.home() / ".scitrans")]
    }
    
    # Only set server_port if we have a specific port (not 0)
    if port and port != 0:
        launch_kwargs["server_port"] = port
        url = f"http://localhost:{port}"
    else:
        # Gradio will assign a port, we'll print it after launch
        url = "http://localhost:<auto-assigned>"
    
    print(f"\n{'='*60}")
    print(f"üöÄ Starting SciTrans GUI...")
    print(f"{'='*60}")
    print(f"üì± GUI will be available at: {url}")
    print(f"üåê If browser doesn't open automatically, visit the URL shown above")
    print(f"{'='*60}\n")
    
    try:
        gui = SciTransGUI()
        app = gui.create_interface()
        
        app.launch(**launch_kwargs)
    except Exception as e:
        print(f"\n‚ùå Error launching GUI: {e}")
        print(f"Please check the error above and try again.")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    launch()
